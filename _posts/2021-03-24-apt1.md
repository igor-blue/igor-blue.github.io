---
layout: single
title:  "APT Encounters of the Third Kind"
---


A few weeks ago an ordinary security assessment turned into an incident response whirlwind. It was definitely a first for me, and I was kindly granted permission to outline the events in this blog post. I\'ll be back to posting about my hardware research soon.

* Do not remove this line (it will not be displayed)
{:toc}

# How it started
Twice a year I am hired to do security assessments for a specific client. We have been working together for several years, and I had a pretty good understanding of their network and what to look for.

This time my POC, Klaus, asked me to focus on privacy issues and GDPR compliance. However, he asked me to first look at their cluster of reverse gateways / load balancers: 

![LB Architecture](/images/im10_network.png)

I had some prior knowledge of these gateways, but decided to start by creating my own test environment first.
The gateways run a custom Linux stack: basically a monolithic compiled kernel (without any modules), and a static GOlang application on top. The 100+ machines have no internal storage, but rather boot from an external USB media that has the kernel and the application. The GOlang app serves in two capacities: an `init` replacement and the reverse gateway software. During initialization it mounts /proc, /sys, devfs and so on, then mounts an NFS share hardcoded in the app. The NFS share contains the app\'s configuration, TLS certificates, blacklist data and a few more. It starts listening on 443, filters incoming communication and passes valid requests on different services in the production segment.

![GW Architecture](/images/im12_gwstack.png)

I set up the test environment, and spent a day in black box examination. Having found nothing much I suggested we move on to looking at the production network, but Klaus insisted I continue with the gateways. Specifically he wanted to know if I could develop a methodology for testing if an attacker has gained access to the gateways and is trying to access PII from within the decrypted HTTP stream.

I couldn\'t SSH into the host (no SSH), so I figured we will have to add some kind of instrumentation to the GO app. But Klaus wanted me to start by looking at the traffic before (red) and after the GW (green), and gave me access to a mirrored port on both sides so I could capture traffic to a standalone laptop he set up and I could access through an LTE modem but was not allowed to upload data from:

![GW Architecture](/images/im13_gwstack_colors2.png)

The main issue I could see was how to correlate the encrypted and decrypted HTTP traffic.  Inspecting the decoded traffic I noticed the GW app adds an \'X-Orig-Connection\' with the four-tuple of the TLS connection! Yay!

![Original connection](/images/im20_sniff80.png)

I quickly wrote a small python program to scan the port 80 traffic capture and create a mapping for each TLS connection - True for connection with PII and False for all others:
```
10.4.254.254,443,[Redacted],43404,376106847.319,False
10.4.254.254,443,[Redacted],52064,376106856.146,False
10.4.254.254,443,[Redacted],40946,376106856.295,False
10.4.254.254,443,[Redacted],48366,376106856.593,False
10.4.254.254,443,[Redacted],48362,376106856.623,True
10.4.254.254,443,[Redacted],45872,376106856.645,False
10.4.254.254,443,[Redacted],40124,376106856.675,False 
...
```

With this in mind I could now extract the data from the PCAPs and do some correlations. After a few long hours getting `scapy` to actually parse timestamps consistently enough for comparisons, I had a list of connection `start`, `end` and `is-PII`. A few more fun hours with Excel and I got histogram graphs of time vs count of packets. Everything looked normal for the HTTP traffic, although I expected more of a normal distribution than the power-low type thingy I got. Port 443 initially looked the same, and I got the normal distribution I expected. But when filtering for PII something was *seriously* wrong. The distribution was skewed and shifted to longer time frames. And there was nothing similar on the port 80 end.

![Histograms](/images/im23_graphs1.png)

My only explanation was that something was wrong with my testing setup (the blue bars) vs. the real live setup (the orange bars). I wrote on our slack channel \'I think my setup is sh\*t, can anyone resend me the config files?\', but this was already very late at night, and no one responded. Having a slight OCD I couldn't let this go. The GW restarted daily, staggered one by one, with about 10 minutes between hosts, and I could see the NFS traffic, so maybe I could get the production configuration files from the NFS capture (bottom dotted blue line in the diagram before).

So to cut a long story short I found the NFS read reply packet, found the **data**, but why the hack is `eof` 77685??? Come on people, its 3:30AM!

What\'s more, the actual data was 77685 bytes, exactly 8192 bytes more then the 'Read length'. The entropy for that data was pretty uniform, suggesting it was encrypted. The file I had was definitely not encrypted.

![First NFS capture](/images/im30_nfs_cap1.png)

Histogram of extra 8192 bytes:

![NFS capture hist](/images/im31_histogram.png)

When I mounted the NFS export myself I got a normal EOF value of `1`!

![NFS capture hist](/images/img33_normal_eof.png)

# What hell is this?

Comparing the capture from my testing machine with the one from the port mirror I saw something else weird:

I snooped around the NFS packets some more and found another weird thing:

![NFS capture hist](/images/im41_nfs1.png)

For other NFS open requests (on all of my test system captures and for other files in the production system) we get:

![NFS capture hist](/images/im42_nfs2.png)

Spot the difference?

The `open id:` string became `open-id:`. Was I dealing with some corrupt packet? But the exact same problem appeared the next time `blacklist.db` was retrieved by another GW rebooting.

Time to look at the source code:

![NFS capture hist](/images/im50_grep.png)

The "open id" string is hardcoded. What\'s up?

After a good night sleep and no beer this time I repeated the experiment and then opened the kernel in IDA.

What I expected to see was this (from nfs4xdr.c):

```c
static inline void encode_openhdr(struct xdr_stream *xdr, const struct nfs_openargs *arg)
{
    __be32 *p;
 /*
 * opcode 4, seqid 4, share_access 4, share_deny 4, clientid 8, ownerlen 4,
 * owner 4 = 32
 */
    encode_nfs4_seqid(xdr, arg->seqid);
    encode_share_access(xdr, arg->share_access);
    p = reserve_space(xdr, 36);
    p = xdr_encode_hyper(p, arg->clientid);
    *p++ = cpu_to_be32(24);
    p = xdr_encode_opaque_fixed(p, "open id:", 8);
    *p++ = cpu_to_be32(arg->server->s_dev);
    *p++ = cpu_to_be32(arg->id.uniquifier);
    xdr_encode_hyper(p, arg->id.create_time);
}
```

Running `binwalk -e -M bzImage` I got the internal ELF image, and opened it in IDA. Of course I didn't have any symbols, but I got `nfs4_xdr_enc_open()` from /proc/kallsyms, and from there to `encode_open()` which led me to `encode_openhdr()`.
The code looked something like 

```c
static inline void encode_openhdr(struct xdr_stream *xdr, const struct nfs_openargs *arg)
{
    ...
    p = xdr_encode_opaque_fixed(p, unknown_func("open id:", arg), 8);
    ...
}
```

The function `unknown_func` was pretty long and complicated but eventually sometimes decided to replace the space between \'open\' and \'id\' with an hyphen.

Does the NFS server care? Apparently it is some opaque client string that gets ignored by the NFS server. OK, back to the weird \'eof\' thingy from the NFS server. 

# The NFS Server

The server was running the \'NFS-ganesha-3.3\' package. This is a very modular user-space NFS server that is implemented as a series of loadable modules called FASLs. For example support for files on the regular filesystem is implemented through a module called `libfsalvfs.so`.
All the files on disk had the same SHA1 as from the distro package, so I decided to dump the process memory. I didn\'t have any tools on the host, so I used GDB which helpfully was already there.  In the middle of taking the dump GDB was suddenly killed, the file I specified got deleted, and the nfs server process restarted. 

I took the dump again but there was nothing there!

I was pretty suspicious at this time, and wanted to recover the original dump file from the first dump. Fortunately for me I dumped it to another NFS server over the network. The file was deleted, but I managed to recover the deleted file from disk on that server.

# 2nd malicious binary

It was truncated, but had a very patched version of NFS-ganesha inside. There were two `libfsalvfs.so` files: the original one and an injected SO file with the same name. The injected file was clearly malicious. The main binary was patched in a few places, and the  function table into `libfsalvfs.so` as replaced with the alternate `libfsalvfs.so`. The alternate file was compiled from NFS-ganesha sources, but modified with two covert channels.

The first one we encountered already:

- When an open request comes in with \'open-id\' instead of \'open id\', the file handle is marked. This is opaque so that unpatched servers just ignore it.
- When the file is read, the last block is appended with a payload from memory, and the \'eof\' value is changed to have the new total size. An unpatched kernel will just ignore it as the EOF value is used as a bool, e.g. checked for 0 or not and not a specific value.

The second covert channel was used for command and control, and was implemented in the VFS code as a fake directory.

Any writes to `/<export>/.snapshot/meta/<cmdid>` are handled by the malware code and not passed on to the FS.
At least the following commands are implemented:
- 1701 - self destruct
- 1702 - set auto self destruct time 
- 1703 - run shell command
- 1704 - load SO file from buffer specified in command 
- 1706 - get basic system description
- 1707 - get network connections
- 170A - upgrade to new SO file
- 74201 - put buffer in memory dict by ID
- 74202 - get buffer from memory dict by ID
- 74650 - put a payload in memory for the first covert channel
- 74651 - arm the first covert channel
- 74652 - disarm the first covert channel

e.g. `cp payload.so /mnt/server/.snapshot/meta/1704` will load the SO file in the NFS ganesha process on the target server, and `echo 1616580589 > /mnt/server/.snapshot/meta/1702` will set an autodestruct time.

The self destruct command (0x1701) is very interesting - it send a UDP broadcast on port 41701 with a random payload of size 321 bytes, and then restarts a clean NFS-ganesha. I guess this is some kind of network signalling.
It appears the malware has a watchdog that iterates over all `/proc/<pid>/fd/*` links every 25ms, and if a link is open to `/proc/<malware pid>/maps` or `/proc/<malware pid>/smaps` or  `/proc/<malware pid>/mem` it:
- kills the offending process,
- deletes any writeable file handle it had opened (before it was killed) in `/proc/<pid>/fd/`, first trying to wipe it,
- calls the self destruct command (0x1701).

# Further forensics
Lets go back to that awful Saturday I triggered the self-destruct. Having a faint understanding I triggered something (that being before I reversed the devil) I asked Klaus to disconnect all the network connections to the outer world and we started taking memory dumps of whatever we could, to a fresh NFS server we setup on a laptop. 
In hindsight we destroyed quiet a bit of evidence that was triggering more self destructs in other subnets, but I think the self destruct has already gone out to the bad guys through a different pace of malware (we later partially recovered) that got the UDP distress signal (that\'s what is was called in the binary, not my naming).

After getting all the forensics the client insisted on reconnecting to the web, they were losing money, and I started reversing. While reversing the malicious `libfsalvfs.so` I discovered the commands I mentioned above, and discovered something that helped me fill more paces of the puzzle.

There was clearly a function I called `get_obfuscated_string(buffer, string_id)`, but it was horrendous, irreversible:

![NFS capture hist](/images/im70_obs1.png)

It had like a billion nested switches:

![NFS capture hist](/images/im71_obs2.png)

I think they let some intern fresh out of college write that one. It seems all possible strings used by the tool are encoded inside in nested switches, with a variable length encoding, e.g. in one branch the 2nd level might have 3 bits and in another it might have 5 and in a third only a single bit. Some kind of prefix tree if I remember anything from Uni.

Eventually I managed to write code to just brute force the function:
```cpp
#include <stdio.h>
#include <stdlib.h>
#include <sys/mman.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <unistd.h>
#include <string>
#include <set>

int main(int argc, char* argv[])
{
	// error handling code omitted
	const char* filename = (argc > 1) ? argv[1] : "reconstructed.elf";
	unsigned long offset = (argc > 1) ? strtol(argv[2], NULL, 16) : 0x22a0;

	int fd = open(filename, O_RDONLY);
	struct stat stbuf;
	fstat(fd, &stbuf);
	const char* addr = (char*)mmap(NULL, stbuf.st_size, PROT_READ | PROT_EXEC, MAP_PRIVATE, fd, 0);
	close(fd);
	const char* base = addr + offset;

	typedef int (*entry_t)(char* outbuf, int id);
	entry_t entry = (entry_t)base;
	std::set<std::string> found;
	char buffer[1024];
	
	for(long bits = 1; bits < 64; ++ bits) {
		bool any_new = false;
		for(long id = (bits == 1) ? 0 : (1 << (bits - 1)); id < (1<<bits); ++ id) {
			int status = entry(buffer, id);
			if(status == 0)
				continue;
			if(found.find(buffer) != found.end())
				continue;
			found.insert(buffer);
			printf("Got '%s'! [0x%x]\n", buffer, id);
			any_new = true;
		}
		if(!any_new)
			break;
	}

	return 0;
}
 ```

This first binary had the following strings (I am keeping 3 to myself as they have client related info):

```
'/proc/self/mem', 
'/proc/self/maps',
'/proc/self/cwd',
'/proc/self/environ',
'/proc/self/fd/%d',
'/proc/self/fdinfo/%d',
'/proc/self/limits',
'/proc/self/cgroup',
'/proc/self/exe',
'/proc/self/cmdline',
'/proc/self/mounts',
'/proc/self/smaps',
'/proc/self/stat',
'/proc/%d/mem', 
'/proc/%d/maps',
'/proc/%d/cwd',
'/proc/%d/environ',
'/proc/%d/fd/%d',
'/proc/%d/fdinfo/%d',
'/proc/%d/limits',
'/proc/%d/cgroup',
'/proc/%d/exe',
'/proc/%d/cmdline',
'/proc/%d/mounts',
'/proc/%d/smaps',
'/proc/%d/stat',        
'nfs',
'nfs4',
'tmpfs',
'devtmpfs',
'procfs',
'sysfs',
'WSL2',
'/etc/os-release',
'/etc/passwd',
'/etc/lsb-release',
'/etc/debian_version',
'/etc/redhat-release',
'/home/%s/.ssh',
'/var/log/wtmp',
'/var/log/syslog',
'/var/log/auth.log',
'/var/log/cron.log',
'/var/log/syslog.log',
'/etc/netplan/*.yaml',
'/etc/yp.conf',
'/var/yp/binding/',
'/etc/krb5.conf',
'/var/kerberos/krb5kdc/kdc.conf',
'/var/log/ganesha.log',
'/etc/ganesha/ganesha.conf',
'/etc/ganesha/exports',
'/etc/exports',
'Error: init failed',
'DELL',
'/usr/lib/x86_64-linux-gnu/libnfs.so.4',
'/tmp/.Test-unix/.fa76c5adb8c04239ff3034106842773b',
'Error: config missing',
'Error: sysdep missing',
'Running',
'LOG',
'/usr/lib/x86_64-linux-gnu/ganesha/libfsalvfs.so',
'none',
'/etc/sudoers',
'/proc/net/tcp',
'/proc/net/udp',
'/etc/selinux/config',
'libdl.so.2',
'libc-',
'.so',
'cluster-config',
'recovery-signal',
```

# Eureka Moment
Looking at this weird function I thought to myself: maybe I can look for code that looks like that in all the dumps I have. We have all those block of `mov byte ptr [rdi+?], '?'`:

![MoveRDI](/images/im72_rdi.png)

So lets look for them:

```c
import sys

with open(sys.argv[1], 'rb') as f:
    data = f.read()

STATE=None
for i in range(len(data) - 6):
    if ord(data[i]) == 0xc6 and ord(data[i + 1]) == 0x47:
        if STATE and (STATE[0] + STATE[1] + 0x40) >= i:
            STATE[1] = i - STATE[0]
            STATE[2] += 1
        else:
            if STATE and STATE[2] >= 20:
                print('Found region at 0x%x - 0x%x' % (STATE[0], STATE[0] + STATE[1]))
            STATE = [i, 4, 1] 
```

And I found them. I did. With some adjustment I found it even on a few ARM systems:

![MoveRDIARM](/images/im72_arm.png)

# The GOlang thingy

I finally found the payload that was sent over to the GW machines. It had 2 stages: the first was the 8192 buffer loaded through the first covert channel. The kernel was modified to inject this buffer into the GOlang application and hook it:

- It allocated a bunch of memory immediately after the GOlang binary - let\'s call it the trampoline area
- Then used the ELF symbol table to find the \'net/http.(\*connReader).Read\' symbol, 
- Copied 1st trampoline function to the trampoline area (see below), patching the area marked with `HERE` with the first 9 bytes of `net/http.(\*connReader).Read`
- `mprotect(net_http_connReader_read & ~0xfff, 8192, PROT_EXEC | PROT_READ | PROT_WRITE)`
- modified the beginning of `net/http.(\*connReader).Read` to a near jump into the trampoline

First trampoline function
```nasm
     pop     rax            
     pop     rcx
     push    rcx
     push    rax
     mov     r11, cs:qword_<relocated>
     mov     rdi, rcx
     call    qword ptr [r11+8]
     pop     rax
     pop     rcx
     push    rcx
     mov     rcx, fs:0FFFFFFFFFFFFFFF8h <---- HERE
     cmp     rsp, [rcx+10h]
     jmp     rax
```

- When the trampoline is called (from the jump in the beginning of `net/http.(*connReader).Read`) it checks if a return trampoline has already been allocated for the return address for the function,
- If not it allocates a new trampoline per call location of `net/http.(*connReader).Read` from the code below, replacing 123456789ABCDEFh with the absolute address of a function in the malware,
- GOlang uses memory for all function argument passing, so immediately after function there is always a 5 byte `mov     reg, [rsp+?]`, so the function copies this `mov` instruction into the first `db 5 dup(0)` area
- replacing it with a near jump to the 2nd trampoline
- and the 2nd db 5 dup(0) is filled with a relative near ```jmp``` back to the original code patch site.
```nasm
      mov     rax, 123456789ABCDEFh
      mov     rdi, rsp
      call    rax
      db 5 dup(0)
      db 5 dup(0)      
```

This way eventually all the `net/http.*connReader).Read` call sites are patched to call a function immediatly after `net/http.(*connReader).Read` returns. This lets the malicous code inspect the decoded HTTP packet.

On initialization the 1st stage malware also loads the hefty 2nd stage through the 2nd covert channel, and passes each buffer received from the patch on `net/http.(*connReader).Read` to it for inspection.
The data collected is collected and compressed by the malware and stored back to the NFS server (the 2nd covert channel which bypasses read ACLs on NFS).

Before this case I did not think there was any nice way to hook random GO binaries, this technique is pretty cool.
Unfortunatly I cannot discuss what the 2nd payload actually as it will reveal stuff my employer isn\'t ready for yet.

# How the kernel got patched? and why not the golang app?
The golang app is built inside a CI segment that can only be accessed through monitored jump hosts with MFA. The CI/CD pipline daily gets the source code from the GIT server, builds it, and automatically tests it in a pre-production segment, and then digitally signs the application and uploads it to the NFS server, and the running APP self updates.

The kernel, on the other hand, is manually built by the guy responsible for it on his own laptop. He then digitally signs it and stores it on a server where it is used by the CI/CD pipeline. Fortunatly for us a commented out line in a script in the CI/CD pipline (a line that was not commented out in the GIT!) did not delete old versions of the kernel and we know which versions were tampered with.

We noticed a 3 month gap about 5 month ago, and it corresponded with the guy moving the kernel build from a Linux laptop to a new Microsoft Surface Book laptop with a VirtualBox VM in it for compiling the kernel. It looks as if it took 3 month for the attackers to gain access back into the box and into the VM build.

# What we have so far

We found a bunch of malware sitting in the network collecting PII information from incoming HTTPS connection after they are decoded in a GOlang app. The data is exfiltrated through the malware network and eventually we guess can get to the bad guys.
We have more info but I am still working on it, expect another blog post in the future with more details, samples, etc'.

# Q&A

- **Q**: What was the initial access vector?

  **A**: We have a pretty good idea, but I cannot publish it yet (RD and stuff). Stay tuned!

- **Q**: Why didn\'t I upload anything to VT yet? 

  **A**: A few reasons:
  - I need to make sure no client info is in the binaries - some of the binaries have hardcoded strings that cannot be shared
   - All of the binaries I have have been reconstructed from memory dumps, so are not in their original form. Does anyone know how to upload partial dumps into VT?

- **Q**: It there a security vulnerability in GO? in the Kernel?

  **A**: Defenitly not! this is just an obnoxious attacker doing what obnoxious attacker do. I might even say the complexity of the stuff means they don't have a 0day for this platform.
  
- **Q**: What about YARA rules, C2 address, etc\'?

  **A**: Wait for it, there is a lot more coming!
  
- **Q**: Why did you publish instead of collecting more? 

  **A**: To quote the client \"I don\'t care who else they are attacking. I just want them off my lawn!\", and he thinks publishing will prevent them from returning to **THIS** network. 
     Hopefully what we publish next time will get them off other people's lawns.

- **Q**: Any Windows malware? 

  **A**: Definitly, including what we believe is an EDR bypass. Still working on it.

- **Q**: Any zero days?

  **A**: Maybe ...
  
**To be continued.**

  
