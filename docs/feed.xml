<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://igor-blue.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://igor-blue.github.io/" rel="alternate" type="text/html" /><updated>2021-02-16T11:33:53-05:00</updated><id>https://igor-blue.github.io/feed.xml</id><title type="html">Igor’s Blog</title><subtitle>I write about my blue team experiences and my hobby researching hardware security.</subtitle><entry><title type="html">Security of the Intel Graphics Stack - Part 1 - Introduction</title><link href="https://igor-blue.github.io/2021/02/10/graphics-part1.html" rel="alternate" type="text/html" title="Security of the Intel Graphics Stack - Part 1 - Introduction" /><published>2021-02-10T00:00:00-05:00</published><updated>2021-02-10T00:00:00-05:00</updated><id>https://igor-blue.github.io/2021/02/10/graphics-part1</id><content type="html" xml:base="https://igor-blue.github.io/2021/02/10/graphics-part1.html">&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#general-architecture&quot; id=&quot;markdown-toc-general-architecture&quot;&gt;General Architecture&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#core-graphics&quot; id=&quot;markdown-toc-core-graphics&quot;&gt;Core Graphics&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#2d-graphics-pipeline&quot; id=&quot;markdown-toc-2d-graphics-pipeline&quot;&gt;2D Graphics Pipeline&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3d-graphics-pipeline&quot; id=&quot;markdown-toc-3d-graphics-pipeline&quot;&gt;3D Graphics Pipeline&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#the-execution-units-eus&quot; id=&quot;markdown-toc-the-execution-units-eus&quot;&gt;The Execution Units (EUs)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-guc&quot; id=&quot;markdown-toc-the-guc&quot;&gt;The GuC&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#boot-rom-and-guc-firmware&quot; id=&quot;markdown-toc-boot-rom-and-guc-firmware&quot;&gt;Boot ROM and GuC firmware&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#the-μos-kernel&quot; id=&quot;markdown-toc-the-μos-kernel&quot;&gt;The μOS kernel&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#communication-with-the-os&quot; id=&quot;markdown-toc-communication-with-the-os&quot;&gt;Communication with the OS&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#host-graphics-architecture&quot; id=&quot;markdown-toc-host-graphics-architecture&quot;&gt;Host Graphics Architecture&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#uefi&quot; id=&quot;markdown-toc-uefi&quot;&gt;UEFI&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#windows&quot; id=&quot;markdown-toc-windows&quot;&gt;Windows&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#basic-memory-management&quot; id=&quot;markdown-toc-basic-memory-management&quot;&gt;Basic Memory Management&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#svm-mode&quot; id=&quot;markdown-toc-svm-mode&quot;&gt;SVM Mode&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#cache-coherence&quot; id=&quot;markdown-toc-cache-coherence&quot;&gt;Cache Coherence&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#boot-process&quot; id=&quot;markdown-toc-boot-process&quot;&gt;Boot process&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#oca&quot; id=&quot;markdown-toc-oca&quot;&gt;OCA&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I promised I’ll post stuff about low level hardware issues, and here is my second post on the subject, the first part in a series about the Intel graphics stack.&lt;/p&gt;

&lt;p&gt;This post series will be a summary of about a decade of unpublished research I am trying to organize and share.
Not all of it is current, as newer hardware is harder to inspect and reverse, but I think much of the research is relevant.&lt;/p&gt;

&lt;p&gt;The first post below is a quick introduction to the different components on the hardware and software side we’ll need to dive into security issues in the next post.&lt;/p&gt;

&lt;h1 id=&quot;general-architecture&quot;&gt;General Architecture&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Processor graphics - The graphics unit that is part of the processor itself. Has had many codenames over the years, HD Graphics, UHD Graphics, Iris, Gen9, Gen11, Intel Xe and so on. Even the ‘Gen’ name has double meaning - both generation and ‘Graphics ENgine’. In UEFI code it is sometimes refered to at the IGD - Integrated Graphics Device.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The GuC - an embedded i486 core that supports graphics scheduling, power management and firmware attestation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;UEFI and OS Drivers&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;core-graphics&quot;&gt;Core Graphics&lt;/h1&gt;
&lt;p&gt;As discussed in the introduction to the SecureBoot post, the Intel CPU has four major component groups - the CPU cores, the L3 (or LLC) cache slices, the ‘Uncore’ or ‘System Agent’ parts, all connected through a ring bus inside the die.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/gen9.png&quot; alt=&quot;Gen9 Architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The graphics process is made up from several &lt;em&gt;slices&lt;/em&gt; and an &lt;em&gt;unslice&lt;/em&gt; (like &lt;em&gt;uncore&lt;/em&gt;) area that includes common components.
Each slice is divided into subslices and a slice common area. The subslices are made up of several Execution Units (EUs), and Texture unit and a L1 Cache/Memory. The common area includes the L3 cache and the dataport.
The limit to the number of slices is the interconnect between them and the unslice. 
There is always only a single &lt;em&gt;unslice&lt;/em&gt;. In the unslice we can find the connection to the ring bus, aptly named the GT interface (GTI), the &lt;em&gt;Command Streamer&lt;/em&gt; is reads commands from the system memory and into the graphics processor, the &lt;em&gt;fixed function pipline&lt;/em&gt; (FF pipeline), and the thread dispatcher \&amp;amp; spawner that lunch shader programs and GPGPU (General Purpose Computing) programs onto the EUs. The FF pipeline deals with fixed functions such as vertice operations (called the Geometry Pipe), and other dedicated hardware such as video transcoding.&lt;/p&gt;

&lt;p&gt;Different SKUs have different combinations of these. For example:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Skylake GT2: 1 slice of 3 subslices of 8 EUs (1x3x8)&lt;/li&gt;
  &lt;li&gt;Skylake GT3: 2x3x8&lt;/li&gt;
  &lt;li&gt;Skylake GT4: 3x3x8&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/gen9slice.png&quot; alt=&quot;Gen9 Architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The graphics engine is also connect straight to the IOSF (Intel On Chip Fabric internal bus, see the &lt;a href=&quot;/2021/02/04/secure-boot.html&quot;&gt;secureboot post&lt;/a&gt; bus, through a controller called Gunit.  Gunit is connect to both the primary and secondary IOSF and exports functions for communicating with the graphics engine and implementing IOMMU support for graphics memory and unified memory.&lt;/p&gt;

&lt;p&gt;All of this is connected to the display IO interconnect and output to DisplayPort and HDMI outputs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/gunit.png&quot; alt=&quot;Gen9 Architecture&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2d-graphics-pipeline&quot;&gt;2D Graphics Pipeline&lt;/h2&gt;
&lt;p&gt;The 2D graphics engine is a standalone IP block in the &lt;em&gt;unslice&lt;/em&gt; area, and has its own command streamer, registers and cache. It has 256 different operation codes, for example:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2dops.png&quot; alt=&quot;2D BitBlt Operations&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3d-graphics-pipeline&quot;&gt;3D Graphics Pipeline&lt;/h2&gt;
&lt;p&gt;The fixed function pipeline in the &lt;em&gt;unslice&lt;/em&gt; implements the DirectX 11 redndering pipeline stages: 
Vertex Fetch -&amp;gt; Vertex Shader -&amp;gt; Hull Shader -&amp;gt; Tessellator -&amp;gt; Domain Shader -&amp;gt; Geometry Shader -&amp;gt; Clipper -&amp;gt; Windower -&amp;gt; Z Ordering, -&amp;gt; Pixel Shader - &amp;gt;Pixel Output. Some of these functions are self contained, but many are implemented using by running shader programs on the EUs in the slices. EUs can send certain operations back into dedicated hardware units.&lt;/p&gt;

&lt;h2 id=&quot;the-execution-units-eus&quot;&gt;The Execution Units (EUs)&lt;/h2&gt;
&lt;p&gt;The EUs are in-order mulithreaded SIMD processing cores. Each execution thread is dispatched has its own 128 register space and executed programs called “kernels”. All instructions are 8 channels wide, e.g. operate on 8 registers at a time (or 16 half registers). Its supports arithmetic, logical and control flow instructions on floats and ints. Registers are addressed by address.
The EU thread dispatcher implements priorities based on age, i.e. oldest is highest priority, and whether the trhead is blocked waiting on instruction fetches, register dependencies etc’. C&lt;/p&gt;

&lt;h1 id=&quot;the-guc&quot;&gt;The GuC&lt;/h1&gt;
&lt;p&gt;The GuC is a small embedded core that supports graphics scheduling, power management and firmware attestation.
It is implemented in an i486DX4 CPU (also called P24C and Minute IA), although it seems that since broadwell it has been extended to the Pentium (i586) ISA. It runs a small microkernel call μOS. 
The GuC μOS runs only kernel level tasks (even though μOS supports μApps). The firmware is written in C with not stdlib.
In the GuC we can find supporting blocks: ROM memory, 8KB L1 on core cache, 64KB/128KB/256KB (Broadwell/Skylake/CannonLake) of SRAM  memory which is used for code+data+cache and a 8KB stack. It also has power management, DMA engine, etc’. 
Communication to the GuC is done through memory-mapped IO and bidirectional interrupts.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/guc.png&quot; alt=&quot;GuC architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The GuC offers a light-weight mechanism for dispatch work the host submits to the GPU. This means the GPU driver does not need to handle dispatch and job queuing, making it much faster. The &lt;em&gt;user mode driver (UMD)&lt;/em&gt; can communicate with the GuC directly when required and bypass the need to context switch the main CPU into kernel mode. The &lt;em&gt;kernel mode driver (KMD)&lt;/em&gt; uses the GuC as a gateway for job submission as well. This simplifies the Kernel and provides a single point where all jobs are submitted.
Communication between the UMD and the GuC is done through shared memory queues.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Why is the GuC interesting? Because I think it can communicate with the CSME, CPU and GPU and everything over the IOSF, and if it has bugs it can be used to gain very privileged access to the system and memory&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;boot-rom-and-guc-firmware&quot;&gt;Boot ROM and GuC firmware&lt;/h2&gt;
&lt;p&gt;At system startup GuC is held at reset state until the UEFI firmware initializes the shared memory region for the GPU. Inside the shared region a special subregion call WOPCM is set aside fur GuC (and HuC) firmware. It then releases the GuC from reset and it in turn starts executing a small non-modifiable Boot ROM (16/32KB in size) that initializes the basic GuC hardware, and waits for an interrupt signalling the firmware has been copied to the WOPCM region.
The GuC firmware is an opaque blob supplied by Intel as part of the GPU KMD, which copies it to the shared memory region (GGTT) and signals the Boot ROM with an interrupt. The bootrom verifies the firmware with a digital signature using a SHA256 hash + PKCSv2.1 RSA signature, and if the test passes copies it to SRAM and starts executing.&lt;/p&gt;

&lt;p&gt;The GUC firmware can be extracted from the graphics driver and reversed. Screenshot of IDA open on the kabylake GuC:
&lt;img src=&quot;/images/gucfirmware.png&quot; alt=&quot;GUC firmware&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The GuC also attest the firmware for the video decoder unit, called &lt;em&gt;HuC&lt;/em&gt;. The HuC is an HEVC/H.265 decoded implement in hardware.&lt;/p&gt;

&lt;h2 id=&quot;the-μos-kernel&quot;&gt;The μOS kernel&lt;/h2&gt;
&lt;p&gt;The μOS kernel runs in 32-bit protected mode, with no paging and old-style segments model (CS, DS, etc’). All code run in ring0. The OS handles HW/SW exceptions and crashes, and supplies debugging and logging services.&lt;/p&gt;

&lt;p&gt;Interrupts are handled through the local APIC - I found interrupts coming from the IOMMU, power management, display interfaces, the GPU and the CPU.&lt;/p&gt;

&lt;p&gt;It runs a single process - which initializes the system and then waits for interrupts/events in a loop.&lt;/p&gt;

&lt;h2 id=&quot;communication-with-the-os&quot;&gt;Communication with the OS&lt;/h2&gt;
&lt;p&gt;Commands are dispatched through a ring buffer work queue. Each work item has a header followed by a command. Once a command is posted the CPU notifies the GuC using a “doorbell” interrupt.&lt;/p&gt;

&lt;p&gt;The Windows kernel mode driver supports GuC debugging by setting a registry key:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;\\REGISTRY\MACHINE\SOFTWARE\Intel\KMD\GuC\\
	GuCEnableUkLogging=1
    
\\REGISTRY\MACHINE\SOFTWARE\Intel\KMD\GuC\\
    GuCLoggingVerbositySelect=0/1/2/3 (low, medium, high, max)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;host-graphics-architecture&quot;&gt;Host Graphics Architecture&lt;/h1&gt;
&lt;p&gt;So far we only discussed hardware. The software part of the graphics stack is divided into three levels: UEFI DXE, kernel mode and user mode.&lt;/p&gt;

&lt;h2 id=&quot;uefi&quot;&gt;UEFI&lt;/h2&gt;
&lt;p&gt;Traditionally VGA support was implemented with a legacy Video VBIOS as an PCI option ROM. In UEFI VBIOS was modified into a DXE driver call the &lt;em&gt;Graphics Output Protocol&lt;/em&gt; (&lt;em&gt;GOP&lt;/em&gt;), which support basic display for the UEFI setup menu and for the OS bootloader. The GOP is supplied by Intel to the UEFI vendor. 
The GOP supplies two basic functions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Changing the graphics mode - resolution, pixel depth, etc’&lt;/li&gt;
  &lt;li&gt;Getting the physical address of the framebuffer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Windows boot-loader uses the GOP to setup a memory mapped video framebuffer before entering VBS, and after the hypervisor and SK are loaded the access by winload is only through the framebuffer without invoking the GOP. Windows also uses the GOP for disabling blue screens.&lt;/p&gt;

&lt;h2 id=&quot;windows&quot;&gt;Windows&lt;/h2&gt;

&lt;p&gt;On Windows, Intel supplies a fairly large graphics driver that implements both the user mode driver (UMD) and kernel mode driver (UMD). Applications using Direct3D communicate through the D3D runtime to the DXGI abstraction interface (in dxgkrnl.sys), which in turn communicated with the KMD. The KMD treats 2D Blt and 3D operations through different pipelines and dispatches the operations to the GPU.&lt;/p&gt;

&lt;p&gt;The GPU driver is riddled with telemetry, but I haven’t figured out yet how much of it is sent automatically to Intel, altough crashes are sent through OCA - Online Crash Analysis.&lt;/p&gt;

&lt;h2 id=&quot;basic-memory-management&quot;&gt;Basic Memory Management&lt;/h2&gt;
&lt;p&gt;A very important job of the graphics drivers (both KMD and UMD) is memory management (GMM). The Graphics Memory space is the virtual memory allocated to the GPU, and is translated using the system pages tables to the physical RAM. The memory contains stuff lime geometry data, textures, etc’. The GPU hardware used Graphics Page Tables (GTTs) to decode virtual addresses supplied by the software graphics memory space into hardware. The use of MMUs and page tables on both ends (sw \&amp;amp; hw) has three main benefits: virtualization, per-process isolated graphics memory and non-contiguous physical memory for better utilization.&lt;/p&gt;

&lt;p&gt;The GTTs come in two variants:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Global GTT - a single one level table mapping directly into system pages. It is managed by the HW and configured in UEFI. The UEFI DXE driver maps the GTT into memory and initializes it. It is also called Graphics Stolen Memory (GSM) and Unified Memory Architecture (UMA), not to be confused with CSME’s UMA.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Per-process GTT (PPGTT). This has changed significantly in the Broadwell graphics engine, so we’ll discuss only the new architecture. Modern PPGTT is basically a mirror of the CPU’s paging model with 4 paging levels.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The GMM part of the KMD handles and tracks graphics allocations, manages the GTTs, caching coherence, stolen memory allocation and something I won’t go into right now called &lt;em&gt;swizzling&lt;/em&gt;. The GMM is essential for performance as it allows memory to be setup by the CPU and then accessed by the GPU directly without copying from system memory to GPU memory.&lt;/p&gt;

&lt;p&gt;Its important to note that in modern system the whole system memory can be used for graphics. The driver reports fictious “dedicated” video memory probably to fix old games.
&lt;img src=&quot;/images/drivermemory.png&quot; alt=&quot;Driver  memory&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Security-wise, the graphis driver needs to make sure user process can gain access only to memory allocated to that process, and is cleared before transferring the memory to a different process.&lt;/p&gt;

&lt;h2 id=&quot;svm-mode&quot;&gt;SVM Mode&lt;/h2&gt;
&lt;p&gt;The Intel GPU have added support for another organic memory model, the OpenCL SVM model. In SVM mode the GPU and CPU share the exact same page table, so data structures can be shared AS-IS between both, including embedded pointers and such. 
Five levels of SVM are supported.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Coarse grained - CPU \&amp;amp; GPU have different buffers&lt;/li&gt;
  &lt;li&gt;Fine grained - CPU \&amp;amp; GPU can share memory buffer&lt;/li&gt;
  &lt;li&gt;Fine grained system - CPU \&amp;amp; GPU share entire system memory&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+-----------------+------------------------------------------------------------------------------+
|                 | Type                                                                         |
+-----------------+-----------------------+--------------------------------+---------------------+
|                 |  Coarse-graind-buffer | Fine-grained buffer            | Fine-grained system |
+-----------------+                       +-----------------+--------------+                     |
| Type            |                       | without atomics | with atomics |                     |
+-----------------+-----------------------+-----------------+--------------+---------------------+
| Shared          | V                     | V               |        V     | V                   |
| virtual         |                       |                 |              |                     |
| address         |                       |                 |              |                     |
| space           |                       |                 |              |                     |
+-----------------+-----------------------+-----------------+--------------+---------------------+
| No need for     |                       | V               |        V     | V                   |
| explicit        |                       |                 |              |                     |
| mapping by host |                       |                 |              |                     |
+-----------------+-----------------------+-----------------+--------------+---------------------+
| Fine-           |                       | V               |        V     | V                   |
| grained         |                       |                 |              |                     |
| coherency       |                       |                 |              |                     |
+-----------------+-----------------------+-----------------+--------------+---------------------+
| Fine-           |                       |                 |        V     | V                   |
| grained         |                       |                 |              |                     |
| synchorinzation |                       |                 |              |                     |
+-----------------+-----------------------+-----------------+--------------+---------------------+
| Implicit use    |                       |                 |              | V                   |
| of memory       |                       |                 |              |                     |
| from CPU        |                       |                 |              |                     |
| malloc() from   |                       |                 |              |                     |
| GPU and entire  |                       |                 |              |                     |
| CPU address     |                       |                 |              |                     |
| space           |                       |                 |              |                     |
+-----------------+-----------------------+-----------------+--------------+---------------------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;cache-coherence&quot;&gt;Cache Coherence&lt;/h2&gt;
&lt;p&gt;Both the CPUs and GPUs have a complex memory hierarchy involving many caches. For example:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CPU: L1 Cache -&amp;gt; L2 Cache -------------\ 
                                       |------&amp;gt; *System LLC Cache -&amp;gt; eDRAM -&amp;gt; RAM 
GPU: Transient Cache -&amp;gt; GPU L3 Cache --/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;GPU memory accesses do not pass through the CPU core’s L1+L2 caches, so the GPU implements &lt;em&gt;snooping&lt;/em&gt; to maintain memory-cache coherency. The GPU basically &lt;em&gt;sniffs&lt;/em&gt; the traffic on the CPU L1/L2 caches, and invalidates its own cache (I think this is relevant only to BigCore CPUs, and on Atom this is optional and very costly).
The GPU’s transient caches are not snoopable by the CPU and must be explicitly flushed. The GPU L3 Cache is snoopable by the CPU on some Intel platforms.&lt;/p&gt;

&lt;h2 id=&quot;boot-process&quot;&gt;Boot process&lt;/h2&gt;
&lt;p&gt;At boot, the operating system and kernel mode drive will detect and query the display devices, initialize a default display topology. 
After boot up, display config request will be sent to KMD and KMD in turn will configure the GEN display hardwires
There are also use cases of display hot-plug during runtime, handled by OS user and kernel mode modules/drivers.&lt;/p&gt;

&lt;p&gt;Once the driver is loaded it DirectX initializes it from DxgkDdiStartDevice() which eventually leads to a function that setups the render table per architecture:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;void setup_render_function_table(HW_DEVICE_EXTENSION *pHwDevExt)
{
    KM_RENDER_CONTEXT   *render_context;

    ...

    switch(get_render_core(pHwDevExt))
    {
    ...
        case GEN3_FAMILY:
            ...
        case GEN4_FAMILY:
            ...
            ...
        case GEN8_FAMILY:
            render_context-&amp;gt;FuncTable.PresentBlt                     = func_Gen6PresentBlt;
            render_context-&amp;gt;FuncTable.PresentFlip                    = func_Gen6PresentFlip;
            render_context-&amp;gt;FuncTable.RenderBegin                    = func_Gen6RenderBegin;
            render_context-&amp;gt;FuncTable.Render                         = func_Gen7Render;
            render_context-&amp;gt;FuncTable.RenderEnd                      = func_Gen6RenderEnd;
            render_context-&amp;gt;FuncTable.GDIRender                      = func_Gen6GDIRender;
            render_context-&amp;gt;FuncTable.BuildPagingBuffer              = func_Gen7BuildPagingBuffer;
            render_context-&amp;gt;FuncTable.SubmitCommand                  = func_Gen8SubmitCommand;
            render_context-&amp;gt;FuncTable.PreemptCommand                 = func_Gen6PreemptCommand;
            render_context-&amp;gt;FuncTable.QueryCurrentFenceIRQL          = func_Gen6QueryCurrentFenceIRQL;
            render_context-&amp;gt;FuncTable.IdleHw                         = func_Gen6IdleHw;
            render_context-&amp;gt;FuncTable.StopHw                         = func_Gen6StopHw;
            render_context-&amp;gt;FuncTable.ResumeHw                       = func_Gen6ResumeHw;
            render_context-&amp;gt;FuncTable.GetMDLToGttSize                = func_GetMdlToUpdateGTTCmdSize;
            render_context-&amp;gt;FuncTable.UpdateMDLToGtt                 = func_MDLToGttUpdateGttCmd;
            render_context-&amp;gt;FuncTable.GetMDLToGttSizeOnePage         = func_GetMdlToUpdateGTTCmdSizeOnePage;
            render_context-&amp;gt;FuncTable.UpdateMDLToGttOnePage          = func_UpdateOneGttEntry;
            ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;oca&quot;&gt;OCA&lt;/h2&gt;
&lt;p&gt;OCA is a mechanism that lets drive store device data and send it through windows update back to the driver vendor. 
There are two cases of failures:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Windows thinks there is a problem and the driver needs to be reloaded (TDR). Windows calls &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DxgkDdiCollectDbgInfo()&lt;/code&gt;, a mechanism that lets drive store device data and send it through windows update back to the driver vendor. The Intel GPU driver can add more then 1MB of data through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DxgkDdiCollectDbgInfo()&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;In case of a blue screen (bugcheck), KmBugcheckSecondaryDumpDataCallback() is called and the driver passes data to it.
After both function the data is converted into an OCA blob using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CreateOCAXXXDivision&lt;/code&gt;, and it is later uploaded to Microsoft and from there to Intel. 
The Intel OCA blob contains lots of system and driver information, including what appears to be an Intel specific unique identifier assigned by the driver to the machnine and can be used for tracking.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;In this post we learned the basic components of the graphics stack. In the next post on the graphics stack we’ll start looking into security implications.&lt;/p&gt;</content><author><name></name></author><summary type="html">General Architecture Core Graphics 2D Graphics Pipeline 3D Graphics Pipeline The Execution Units (EUs) The GuC Boot ROM and GuC firmware The μOS kernel Communication with the OS Host Graphics Architecture UEFI Windows Basic Memory Management SVM Mode Cache Coherence Boot process OCA Conclusion I promised I’ll post stuff about low level hardware issues, and here is my second post on the subject, the first part in a series about the Intel graphics stack. This post series will be a summary of about a decade of unpublished research I am trying to organize and share. Not all of it is current, as newer hardware is harder to inspect and reverse, but I think much of the research is relevant. The first post below is a quick introduction to the different components on the hardware and software side we’ll need to dive into security issues in the next post. General Architecture Processor graphics - The graphics unit that is part of the processor itself. Has had many codenames over the years, HD Graphics, UHD Graphics, Iris, Gen9, Gen11, Intel Xe and so on. Even the ‘Gen’ name has double meaning - both generation and ‘Graphics ENgine’. In UEFI code it is sometimes refered to at the IGD - Integrated Graphics Device. The GuC - an embedded i486 core that supports graphics scheduling, power management and firmware attestation. UEFI and OS Drivers Core Graphics As discussed in the introduction to the SecureBoot post, the Intel CPU has four major component groups - the CPU cores, the L3 (or LLC) cache slices, the ‘Uncore’ or ‘System Agent’ parts, all connected through a ring bus inside the die. The graphics process is made up from several slices and an unslice (like uncore) area that includes common components. Each slice is divided into subslices and a slice common area. The subslices are made up of several Execution Units (EUs), and Texture unit and a L1 Cache/Memory. The common area includes the L3 cache and the dataport. The limit to the number of slices is the interconnect between them and the unslice. There is always only a single unslice. In the unslice we can find the connection to the ring bus, aptly named the GT interface (GTI), the Command Streamer is reads commands from the system memory and into the graphics processor, the fixed function pipline (FF pipeline), and the thread dispatcher \&amp;amp; spawner that lunch shader programs and GPGPU (General Purpose Computing) programs onto the EUs. The FF pipeline deals with fixed functions such as vertice operations (called the Geometry Pipe), and other dedicated hardware such as video transcoding. Different SKUs have different combinations of these. For example: Skylake GT2: 1 slice of 3 subslices of 8 EUs (1x3x8) Skylake GT3: 2x3x8 Skylake GT4: 3x3x8 The graphics engine is also connect straight to the IOSF (Intel On Chip Fabric internal bus, see the secureboot post bus, through a controller called Gunit. Gunit is connect to both the primary and secondary IOSF and exports functions for communicating with the graphics engine and implementing IOMMU support for graphics memory and unified memory. All of this is connected to the display IO interconnect and output to DisplayPort and HDMI outputs. 2D Graphics Pipeline The 2D graphics engine is a standalone IP block in the unslice area, and has its own command streamer, registers and cache. It has 256 different operation codes, for example: 3D Graphics Pipeline The fixed function pipeline in the unslice implements the DirectX 11 redndering pipeline stages: Vertex Fetch -&amp;gt; Vertex Shader -&amp;gt; Hull Shader -&amp;gt; Tessellator -&amp;gt; Domain Shader -&amp;gt; Geometry Shader -&amp;gt; Clipper -&amp;gt; Windower -&amp;gt; Z Ordering, -&amp;gt; Pixel Shader - &amp;gt;Pixel Output. Some of these functions are self contained, but many are implemented using by running shader programs on the EUs in the slices. EUs can send certain operations back into dedicated hardware units. The Execution Units (EUs) The EUs are in-order mulithreaded SIMD processing cores. Each execution thread is dispatched has its own 128 register space and executed programs called “kernels”. All instructions are 8 channels wide, e.g. operate on 8 registers at a time (or 16 half registers). Its supports arithmetic, logical and control flow instructions on floats and ints. Registers are addressed by address. The EU thread dispatcher implements priorities based on age, i.e. oldest is highest priority, and whether the trhead is blocked waiting on instruction fetches, register dependencies etc’. C The GuC The GuC is a small embedded core that supports graphics scheduling, power management and firmware attestation. It is implemented in an i486DX4 CPU (also called P24C and Minute IA), although it seems that since broadwell it has been extended to the Pentium (i586) ISA. It runs a small microkernel call μOS. The GuC μOS runs only kernel level tasks (even though μOS supports μApps). The firmware is written in C with not stdlib. In the GuC we can find supporting blocks: ROM memory, 8KB L1 on core cache, 64KB/128KB/256KB (Broadwell/Skylake/CannonLake) of SRAM memory which is used for code+data+cache and a 8KB stack. It also has power management, DMA engine, etc’. Communication to the GuC is done through memory-mapped IO and bidirectional interrupts. The GuC offers a light-weight mechanism for dispatch work the host submits to the GPU. This means the GPU driver does not need to handle dispatch and job queuing, making it much faster. The user mode driver (UMD) can communicate with the GuC directly when required and bypass the need to context switch the main CPU into kernel mode. The kernel mode driver (KMD) uses the GuC as a gateway for job submission as well. This simplifies the Kernel and provides a single point where all jobs are submitted. Communication between the UMD and the GuC is done through shared memory queues. Why is the GuC interesting? Because I think it can communicate with the CSME, CPU and GPU and everything over the IOSF, and if it has bugs it can be used to gain very privileged access to the system and memory. Boot ROM and GuC firmware At system startup GuC is held at reset state until the UEFI firmware initializes the shared memory region for the GPU. Inside the shared region a special subregion call WOPCM is set aside fur GuC (and HuC) firmware. It then releases the GuC from reset and it in turn starts executing a small non-modifiable Boot ROM (16/32KB in size) that initializes the basic GuC hardware, and waits for an interrupt signalling the firmware has been copied to the WOPCM region. The GuC firmware is an opaque blob supplied by Intel as part of the GPU KMD, which copies it to the shared memory region (GGTT) and signals the Boot ROM with an interrupt. The bootrom verifies the firmware with a digital signature using a SHA256 hash + PKCSv2.1 RSA signature, and if the test passes copies it to SRAM and starts executing. The GUC firmware can be extracted from the graphics driver and reversed. Screenshot of IDA open on the kabylake GuC: The GuC also attest the firmware for the video decoder unit, called HuC. The HuC is an HEVC/H.265 decoded implement in hardware. The μOS kernel The μOS kernel runs in 32-bit protected mode, with no paging and old-style segments model (CS, DS, etc’). All code run in ring0. The OS handles HW/SW exceptions and crashes, and supplies debugging and logging services. Interrupts are handled through the local APIC - I found interrupts coming from the IOMMU, power management, display interfaces, the GPU and the CPU. It runs a single process - which initializes the system and then waits for interrupts/events in a loop. Communication with the OS Commands are dispatched through a ring buffer work queue. Each work item has a header followed by a command. Once a command is posted the CPU notifies the GuC using a “doorbell” interrupt. The Windows kernel mode driver supports GuC debugging by setting a registry key: \\REGISTRY\MACHINE\SOFTWARE\Intel\KMD\GuC\\ GuCEnableUkLogging=1 \\REGISTRY\MACHINE\SOFTWARE\Intel\KMD\GuC\\ GuCLoggingVerbositySelect=0/1/2/3 (low, medium, high, max) Host Graphics Architecture So far we only discussed hardware. The software part of the graphics stack is divided into three levels: UEFI DXE, kernel mode and user mode. UEFI Traditionally VGA support was implemented with a legacy Video VBIOS as an PCI option ROM. In UEFI VBIOS was modified into a DXE driver call the Graphics Output Protocol (GOP), which support basic display for the UEFI setup menu and for the OS bootloader. The GOP is supplied by Intel to the UEFI vendor. The GOP supplies two basic functions: Changing the graphics mode - resolution, pixel depth, etc’ Getting the physical address of the framebuffer The Windows boot-loader uses the GOP to setup a memory mapped video framebuffer before entering VBS, and after the hypervisor and SK are loaded the access by winload is only through the framebuffer without invoking the GOP. Windows also uses the GOP for disabling blue screens. Windows On Windows, Intel supplies a fairly large graphics driver that implements both the user mode driver (UMD) and kernel mode driver (UMD). Applications using Direct3D communicate through the D3D runtime to the DXGI abstraction interface (in dxgkrnl.sys), which in turn communicated with the KMD. The KMD treats 2D Blt and 3D operations through different pipelines and dispatches the operations to the GPU. The GPU driver is riddled with telemetry, but I haven’t figured out yet how much of it is sent automatically to Intel, altough crashes are sent through OCA - Online Crash Analysis. Basic Memory Management A very important job of the graphics drivers (both KMD and UMD) is memory management (GMM). The Graphics Memory space is the virtual memory allocated to the GPU, and is translated using the system pages tables to the physical RAM. The memory contains stuff lime geometry data, textures, etc’. The GPU hardware used Graphics Page Tables (GTTs) to decode virtual addresses supplied by the software graphics memory space into hardware. The use of MMUs and page tables on both ends (sw \&amp;amp; hw) has three main benefits: virtualization, per-process isolated graphics memory and non-contiguous physical memory for better utilization. The GTTs come in two variants: Global GTT - a single one level table mapping directly into system pages. It is managed by the HW and configured in UEFI. The UEFI DXE driver maps the GTT into memory and initializes it. It is also called Graphics Stolen Memory (GSM) and Unified Memory Architecture (UMA), not to be confused with CSME’s UMA. Per-process GTT (PPGTT). This has changed significantly in the Broadwell graphics engine, so we’ll discuss only the new architecture. Modern PPGTT is basically a mirror of the CPU’s paging model with 4 paging levels. The GMM part of the KMD handles and tracks graphics allocations, manages the GTTs, caching coherence, stolen memory allocation and something I won’t go into right now called swizzling. The GMM is essential for performance as it allows memory to be setup by the CPU and then accessed by the GPU directly without copying from system memory to GPU memory. Its important to note that in modern system the whole system memory can be used for graphics. The driver reports fictious “dedicated” video memory probably to fix old games. Security-wise, the graphis driver needs to make sure user process can gain access only to memory allocated to that process, and is cleared before transferring the memory to a different process. SVM Mode The Intel GPU have added support for another organic memory model, the OpenCL SVM model. In SVM mode the GPU and CPU share the exact same page table, so data structures can be shared AS-IS between both, including embedded pointers and such. Five levels of SVM are supported. Coarse grained - CPU \&amp;amp; GPU have different buffers Fine grained - CPU \&amp;amp; GPU can share memory buffer Fine grained system - CPU \&amp;amp; GPU share entire system memory +-----------------+------------------------------------------------------------------------------+ | | Type | +-----------------+-----------------------+--------------------------------+---------------------+ | | Coarse-graind-buffer | Fine-grained buffer | Fine-grained system | +-----------------+ +-----------------+--------------+ | | Type | | without atomics | with atomics | | +-----------------+-----------------------+-----------------+--------------+---------------------+ | Shared | V | V | V | V | | virtual | | | | | | address | | | | | | space | | | | | +-----------------+-----------------------+-----------------+--------------+---------------------+ | No need for | | V | V | V | | explicit | | | | | | mapping by host | | | | | +-----------------+-----------------------+-----------------+--------------+---------------------+ | Fine- | | V | V | V | | grained | | | | | | coherency | | | | | +-----------------+-----------------------+-----------------+--------------+---------------------+ | Fine- | | | V | V | | grained | | | | | | synchorinzation | | | | | +-----------------+-----------------------+-----------------+--------------+---------------------+ | Implicit use | | | | V | | of memory | | | | | | from CPU | | | | | | malloc() from | | | | | | GPU and entire | | | | | | CPU address | | | | | | space | | | | | +-----------------+-----------------------+-----------------+--------------+---------------------+ Cache Coherence Both the CPUs and GPUs have a complex memory hierarchy involving many caches. For example: CPU: L1 Cache -&amp;gt; L2 Cache -------------\ |------&amp;gt; *System LLC Cache -&amp;gt; eDRAM -&amp;gt; RAM GPU: Transient Cache -&amp;gt; GPU L3 Cache --/ GPU memory accesses do not pass through the CPU core’s L1+L2 caches, so the GPU implements snooping to maintain memory-cache coherency. The GPU basically sniffs the traffic on the CPU L1/L2 caches, and invalidates its own cache (I think this is relevant only to BigCore CPUs, and on Atom this is optional and very costly). The GPU’s transient caches are not snoopable by the CPU and must be explicitly flushed. The GPU L3 Cache is snoopable by the CPU on some Intel platforms. Boot process At boot, the operating system and kernel mode drive will detect and query the display devices, initialize a default display topology. After boot up, display config request will be sent to KMD and KMD in turn will configure the GEN display hardwires There are also use cases of display hot-plug during runtime, handled by OS user and kernel mode modules/drivers. Once the driver is loaded it DirectX initializes it from DxgkDdiStartDevice() which eventually leads to a function that setups the render table per architecture: void setup_render_function_table(HW_DEVICE_EXTENSION *pHwDevExt) { KM_RENDER_CONTEXT *render_context; ... switch(get_render_core(pHwDevExt)) { ... case GEN3_FAMILY: ... case GEN4_FAMILY: ... ... case GEN8_FAMILY: render_context-&amp;gt;FuncTable.PresentBlt = func_Gen6PresentBlt; render_context-&amp;gt;FuncTable.PresentFlip = func_Gen6PresentFlip; render_context-&amp;gt;FuncTable.RenderBegin = func_Gen6RenderBegin; render_context-&amp;gt;FuncTable.Render = func_Gen7Render; render_context-&amp;gt;FuncTable.RenderEnd = func_Gen6RenderEnd; render_context-&amp;gt;FuncTable.GDIRender = func_Gen6GDIRender; render_context-&amp;gt;FuncTable.BuildPagingBuffer = func_Gen7BuildPagingBuffer; render_context-&amp;gt;FuncTable.SubmitCommand = func_Gen8SubmitCommand; render_context-&amp;gt;FuncTable.PreemptCommand = func_Gen6PreemptCommand; render_context-&amp;gt;FuncTable.QueryCurrentFenceIRQL = func_Gen6QueryCurrentFenceIRQL; render_context-&amp;gt;FuncTable.IdleHw = func_Gen6IdleHw; render_context-&amp;gt;FuncTable.StopHw = func_Gen6StopHw; render_context-&amp;gt;FuncTable.ResumeHw = func_Gen6ResumeHw; render_context-&amp;gt;FuncTable.GetMDLToGttSize = func_GetMdlToUpdateGTTCmdSize; render_context-&amp;gt;FuncTable.UpdateMDLToGtt = func_MDLToGttUpdateGttCmd; render_context-&amp;gt;FuncTable.GetMDLToGttSizeOnePage = func_GetMdlToUpdateGTTCmdSizeOnePage; render_context-&amp;gt;FuncTable.UpdateMDLToGttOnePage = func_UpdateOneGttEntry; ... OCA OCA is a mechanism that lets drive store device data and send it through windows update back to the driver vendor. There are two cases of failures: Windows thinks there is a problem and the driver needs to be reloaded (TDR). Windows calls DxgkDdiCollectDbgInfo(), a mechanism that lets drive store device data and send it through windows update back to the driver vendor. The Intel GPU driver can add more then 1MB of data through DxgkDdiCollectDbgInfo(). In case of a blue screen (bugcheck), KmBugcheckSecondaryDumpDataCallback() is called and the driver passes data to it. After both function the data is converted into an OCA blob using CreateOCAXXXDivision, and it is later uploaded to Microsoft and from there to Intel. The Intel OCA blob contains lots of system and driver information, including what appears to be an Intel specific unique identifier assigned by the driver to the machnine and can be used for tracking. Conclusion In this post we learned the basic components of the graphics stack. In the next post on the graphics stack we’ll start looking into security implications.</summary></entry><entry><title type="html">Analysis of SSH keys found in the wild</title><link href="https://igor-blue.github.io/2021/02/08/ssh-keys-in-the-wild.html" rel="alternate" type="text/html" title="Analysis of SSH keys found in the wild" /><published>2021-02-08T00:00:00-05:00</published><updated>2021-02-08T00:00:00-05:00</updated><id>https://igor-blue.github.io/2021/02/08/ssh-keys-in-the-wild</id><content type="html" xml:base="https://igor-blue.github.io/2021/02/08/ssh-keys-in-the-wild.html">&lt;p&gt;In 2018 I was contracted to help a large organization with a very distributed and remote structure. One of the things that I found was that the organization does not have a strict policy regarding the creation, storage and lifecycle of SSH keys.&lt;/p&gt;

&lt;p&gt;I decided to look into this issue in general, so in Feb 2019 wrote a crawler that looked for SSH keys around the web - public repos, s3 bucket with bad permissions, data dumps from companies and so on.&lt;/p&gt;

&lt;p&gt;From this I got 4807 keys. Next I wrote a small python script that tried the SSH keys - just autenticate and close the connection, without opening any channels as to not actually access the target systems which would be illegal.&lt;/p&gt;

&lt;p&gt;I managed to authenticate into 221 hosts, 5 were FreeBSD, 1 was MacOS, 3 were Linux on ARM64, and the rest were Linux x64. This means I have 221 working keys found on the web and no way to notify their owners they should change their keys.&lt;/p&gt;

&lt;p&gt;General interesting statistics:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Of the 4807 keys 966 were malformed and 1036 were encrypted (20%). Of the 1036 encrypted I could break 88 passwords using dictionaries and an additional 41 passwords using John-the-ripper on a 3-year old 8-core Xeon workstation after a month of brute-forcing.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Sizes (all were SHA256):
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@DESKTOP-MR4OQPJ:~/keys# for i in id_rsa* ; do ssh-keygen -l -f $i; done | sed 's/:.*//' | sort | uniq -c | sort -n -k 2
    2 1023 SHA256
   37 1024 SHA256
    1 2047 SHA256
 2187 2048 SHA256
    1 3000 SHA256
    1 4048 SHA256
  572 4096 SHA256
    3 8192 SHA256
    1 16384 SHA256
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;I don’‘t get the wird sizes: 1023-bit, 2047-bit, 3000-bit, and 4048-bit. Anyone have an idea?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Encryption type:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@DESKTOP-MR4OQPJ:~/enc# grep -h DEK-Info id_rsa* | sed 's/,.*//' | sort | uniq -c
  665 DEK-Info: AES-128-CBC
    2 DEK-Info: AES-256-CBC
   94 DEK-Info: DES-EDE3-CBC
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;Why still use DES keys?&lt;/p&gt;

    &lt;p&gt;for keys that I could not break:&lt;/p&gt;
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  531 DEK-Info: AES-128-CBC
    2 DEK-Info: AES-256-CBC
   66 DEK-Info: DES-EDE3-CBC
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Distributions (in 2019, from uname)&lt;/li&gt;
  &lt;li&gt;87 were Ubuntu&lt;/li&gt;
  &lt;li&gt;38 were RHEL/Centos 6&lt;/li&gt;
  &lt;li&gt;25 were RHEL/Centos 7&lt;/li&gt;
  &lt;li&gt;7 were Amazon&lt;/li&gt;
  &lt;li&gt;5 were RHEL/Centos 5&lt;/li&gt;
  &lt;li&gt;2 were Debian&lt;/li&gt;
  &lt;li&gt;2 were CoreOS&lt;/li&gt;
  &lt;li&gt;1 was Gentoo&lt;/li&gt;
  &lt;li&gt;1 was Fedore32&lt;/li&gt;
  &lt;li&gt;2 were armv7l&lt;/li&gt;
  &lt;li&gt;1 was armv5tel&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;the rest I could not identify from uname -a&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Most common kernels (in 2019, from uname)&lt;/li&gt;
  &lt;li&gt;44 were Linux 2.6.x&lt;/li&gt;
  &lt;li&gt;39 were Linux 4.4.x&lt;/li&gt;
  &lt;li&gt;28 were Linux 4.15.x&lt;/li&gt;
  &lt;li&gt;35 were Linux 3.10.x&lt;/li&gt;
  &lt;li&gt;15 were Linux 3.13.x&lt;/li&gt;
  &lt;li&gt;13 were Linux 4.9.x&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Last week (after two years!) I reran the test against the 221 working keys and 179 still work. To make sure these are not honepots I added to the testing script a checked for the length of the remote .bash_history file, and none seem to be honeypots.&lt;/p&gt;</content><author><name></name></author><summary type="html">In 2018 I was contracted to help a large organization with a very distributed and remote structure. One of the things that I found was that the organization does not have a strict policy regarding the creation, storage and lifecycle of SSH keys. I decided to look into this issue in general, so in Feb 2019 wrote a crawler that looked for SSH keys around the web - public repos, s3 bucket with bad permissions, data dumps from companies and so on. From this I got 4807 keys. Next I wrote a small python script that tried the SSH keys - just autenticate and close the connection, without opening any channels as to not actually access the target systems which would be illegal. I managed to authenticate into 221 hosts, 5 were FreeBSD, 1 was MacOS, 3 were Linux on ARM64, and the rest were Linux x64. This means I have 221 working keys found on the web and no way to notify their owners they should change their keys. General interesting statistics: Of the 4807 keys 966 were malformed and 1036 were encrypted (20%). Of the 1036 encrypted I could break 88 passwords using dictionaries and an additional 41 passwords using John-the-ripper on a 3-year old 8-core Xeon workstation after a month of brute-forcing. Sizes (all were SHA256): root@DESKTOP-MR4OQPJ:~/keys# for i in id_rsa* ; do ssh-keygen -l -f $i; done | sed 's/:.*//' | sort | uniq -c | sort -n -k 2 2 1023 SHA256 37 1024 SHA256 1 2047 SHA256 2187 2048 SHA256 1 3000 SHA256 1 4048 SHA256 572 4096 SHA256 3 8192 SHA256 1 16384 SHA256 I don’‘t get the wird sizes: 1023-bit, 2047-bit, 3000-bit, and 4048-bit. Anyone have an idea? Encryption type: root@DESKTOP-MR4OQPJ:~/enc# grep -h DEK-Info id_rsa* | sed 's/,.*//' | sort | uniq -c 665 DEK-Info: AES-128-CBC 2 DEK-Info: AES-256-CBC 94 DEK-Info: DES-EDE3-CBC Why still use DES keys? for keys that I could not break: 531 DEK-Info: AES-128-CBC 2 DEK-Info: AES-256-CBC 66 DEK-Info: DES-EDE3-CBC Distributions (in 2019, from uname) 87 were Ubuntu 38 were RHEL/Centos 6 25 were RHEL/Centos 7 7 were Amazon 5 were RHEL/Centos 5 2 were Debian 2 were CoreOS 1 was Gentoo 1 was Fedore32 2 were armv7l 1 was armv5tel the rest I could not identify from uname -a Most common kernels (in 2019, from uname) 44 were Linux 2.6.x 39 were Linux 4.4.x 28 were Linux 4.15.x 35 were Linux 3.10.x 15 were Linux 3.13.x 13 were Linux 4.9.x Last week (after two years!) I reran the test against the 221 working keys and 179 still work. To make sure these are not honepots I added to the testing script a checked for the length of the remote .bash_history file, and none seem to be honeypots.</summary></entry><entry><title type="html">Abusing Sybase for lateral movement</title><link href="https://igor-blue.github.io/2021/02/07/sybase.html" rel="alternate" type="text/html" title="Abusing Sybase for lateral movement" /><published>2021-02-07T00:00:00-05:00</published><updated>2021-02-07T00:00:00-05:00</updated><id>https://igor-blue.github.io/2021/02/07/sybase</id><content type="html" xml:base="https://igor-blue.github.io/2021/02/07/sybase.html">&lt;p&gt;A few years ago I was asked to help on a red-team exercise in a company doing hardware R&amp;amp;D.&lt;/p&gt;

&lt;p&gt;The company had a very strict password policy, and every computer had a randomized local adminsitrator account password and local SMB server disabled.&lt;/p&gt;

&lt;p&gt;We managed to gain access to one developer but got stuck there. We did find one thing though: many of the developers had Sybase Adaptive SQL server installed on their systems as it was bundled by default with LabVIEW and Siemens Step 7, both in use by the target.&lt;/p&gt;

&lt;p&gt;I installed LabVIEW and tried accessing it through the Adaptive SQL client. Looking through the connect dialog I notice something interesting: one of the options was &quot;Start and connect to a database on another computer&quot;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/sybase2.png&quot; alt=&quot;Sybase connect dialog&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When selecting this option you need to specify the DB filename. I tried specifying an SMB server and could and pressed &quot;Connnect&quot;. Amazingly, the target computer connected back over an SMB null session to the share I specified. I setup a Samba server that allows anonymous access and placed a DB file I crafted with credentials I specified during creation. This time I managed to connect and execute SQL statments against my server. What was more interesting, the account permissions and roles were set by the DB file and not by the host, so I could setup in advance in my DB to have an administrator role and then I could execute &quot;xp_cmdshell&quot; on the remote host.&lt;/p&gt;

&lt;p&gt;We tried this in the field using ssh port forwarding back home on 445 and got access to most developer computers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/sybase.png&quot; alt=&quot;Sybase login dialog&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This was quiet a few years ago, but looking over the CVE DB for Sybase I don't see any issue that sounds like that, so I guess if you encounter Step7 or LabVIEW during a pentest you now know what to do …&lt;/p&gt;</content><author><name></name></author><summary type="html">A few years ago I was asked to help on a red-team exercise in a company doing hardware R&amp;amp;D.</summary></entry><entry><title type="html">In-depth dive into the security features of the Intel/Windows platform secure boot process</title><link href="https://igor-blue.github.io/2021/02/04/secure-boot.html" rel="alternate" type="text/html" title="In-depth dive into the security features of the Intel/Windows platform secure boot process" /><published>2021-02-04T00:00:00-05:00</published><updated>2021-02-04T00:00:00-05:00</updated><id>https://igor-blue.github.io/2021/02/04/secure-boot</id><content type="html" xml:base="https://igor-blue.github.io/2021/02/04/secure-boot.html">&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction-and-system-architecture&quot; id=&quot;markdown-toc-introduction-and-system-architecture&quot;&gt;Introduction and System Architecture&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#buses&quot; id=&quot;markdown-toc-buses&quot;&gt;Buses&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#more-components&quot; id=&quot;markdown-toc-more-components&quot;&gt;More Components&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#the-flash-chip&quot; id=&quot;markdown-toc-the-flash-chip&quot;&gt;The Flash Chip&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#overview&quot; id=&quot;markdown-toc-overview&quot;&gt;Overview&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#early-power-on&quot; id=&quot;markdown-toc-early-power-on&quot;&gt;Early power on&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bring-up-bup&quot; id=&quot;markdown-toc-bring-up-bup&quot;&gt;Bring-Up (BUP)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#cpu-initialization&quot; id=&quot;markdown-toc-cpu-initialization&quot;&gt;CPU initialization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#uefi-initialization&quot; id=&quot;markdown-toc-uefi-initialization&quot;&gt;UEFI initialization&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#loading-the-boot-loader&quot; id=&quot;markdown-toc-loading-the-boot-loader&quot;&gt;Loading the boot loader&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#windows-boot&quot; id=&quot;markdown-toc-windows-boot&quot;&gt;Windows Boot&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#winload&quot; id=&quot;markdown-toc-winload&quot;&gt;Winload&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#hvci&quot; id=&quot;markdown-toc-hvci&quot;&gt;HVCI&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#dynamic-root-of-trust-model-drtm&quot; id=&quot;markdown-toc-dynamic-root-of-trust-model-drtm&quot;&gt;Dynamic Root of Trust Model (DRTM)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#uefi-memory-attributes-table&quot; id=&quot;markdown-toc-uefi-memory-attributes-table&quot;&gt;UEFI Memory Attributes Table&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#other-oss&quot; id=&quot;markdown-toc-other-oss&quot;&gt;Other OSs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#more-protections&quot; id=&quot;markdown-toc-more-protections&quot;&gt;More Protections&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#iommu-and-dma-protections&quot; id=&quot;markdown-toc-iommu-and-dma-protections&quot;&gt;IOMMU and DMA protections&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#secure-devices&quot; id=&quot;markdown-toc-secure-devices&quot;&gt;Secure Devices&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#smm&quot; id=&quot;markdown-toc-smm&quot;&gt;SMM&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#memory-reset-protections&quot; id=&quot;markdown-toc-memory-reset-protections&quot;&gt;Memory Reset protections&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This blog post is an in-depth dive into the security features of the Intel/Windows platform boot process. In this post I'll explain the startup process through security focused lenses, next post we'll dive into several known attacks and how they were handled by Intel and Microsoft. My wish is to explain to technology professionals not deep into platform security why Microsoft's SecureCore is so important and necessary.&lt;/p&gt;

&lt;h1 id=&quot;introduction-and-system-architecture&quot;&gt;Introduction and System Architecture&lt;/h1&gt;

&lt;p&gt;We must first begin with a brief introduction to the hardware platform. Skip this if you have read the awsome material available on the web about the Intel architecture, I'll try to briefly summarize it here.&lt;/p&gt;

&lt;p&gt;The Intel platform is based on one or two chips. Small systems have one, the desktop and server ones are separated to a CPU complex and a PCH complex (PCH = Platform Controller Hub).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/image2020-4-19_14-3-0.png&quot; alt=&quot;Intel architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The CPU complex deals with computation. It holds the &quot;processor&quot; cores, e.g. Sunny Cove that implement the ISA, as well as cross core caches like the L3 cache, and more controllers that are grouped together as &quot;the system agent&quot; or the &quot;uncore&quot;. The uncore contains the memory controller and display, e.g. GPU and display controller.&lt;/p&gt;

&lt;p&gt;The PCH handles all other IO, including access to the firmware through SPI or eSPI, wifi, LAN, USB, HD audio, SMBus, thunderbolt and etc'. The PCH also hosts several embedded processors, like the PMC, the Power Management Controller.&lt;/p&gt;

&lt;p&gt;An additional part of the PCH is a very important player in our story, the CSME, or Converged Security &amp;amp; Management Engine, a i486 IP block (also called Minute IA). CSME is responsibly for much of the security model of Intel processors as well as many of the manageability features of the platform. The CSME block has its own dedicated ~1.5mb of SRAM memory and 128KB of ROM, as well as a dedicated IOMMU, called the A-Unit (that even has its own &lt;em&gt;acode&lt;/em&gt; microcode) located in the CSME's &lt;em&gt;uncore&lt;/em&gt;', thats allows access from ME to the main memory, as well as DMA to/from the main memory and using the main memory as an encrypted paging area (&quot;virtual memory&quot;). The CSME engine runs a customized version of the Minix3 microkernel, also recent versions have changed it beyond recognition adding many security features.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/image2020-4-19_14-14-4.png&quot; alt=&quot;CSME structure&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;buses&quot;&gt;Buses&lt;/h2&gt;
&lt;p&gt;Lets use this post to also introduce the main interconnects in the system. The main externally facing interconnect bus is PCI-E, a fast bust that can reach 64GBps in its latest incarnations. A second external bus is the LPC, or Low Pin Count bus, a slow bus for connecting devices such as SPI flash, the TPM (explained below), and old peripherals such as PS/2 touchpads.&lt;/p&gt;

&lt;p&gt;Internally the platform is based around the &lt;strong&gt;IOSF&lt;/strong&gt;, or Intel On-chip System Fabric, which is a pumped up version of PCI-E that supports many additional security and addressing features. For addressing IOSF adds SourceID and DestID fields that contain the source and destination of any IOSF transaction, extending PCI-E Bus-Device-Function (BDF) addressing to enable routing over bridges. IOSF also extends addressing by adding support for multiple address root namespaces, currently defining three: RS0 for host memory space, RS1 for CSME memory space, and RS2 for the Innovation-Engine (IE), another embedded controller currently present only on server chipsets.
There are two IOSF busses in the PCH - the Primary Fabric and the Sideband Fabric. The Primary Fabric is high speed, connecting the CPU to the PCH (through a protocol call DMI), as well as high speed devices such as Gigbait Ethernet, WiFi and eSPI. The Sideband Fabric is used to connect the CSME to low-speed devices, including the PMC (Power Management Controller), the RNG generator, GPIO pins, USB, SMBus, and even debugging interfaces such as JTAG.&lt;/p&gt;

&lt;h2 id=&quot;more-components&quot;&gt;More Components&lt;/h2&gt;
&lt;p&gt;Another interesting component is the &lt;strong&gt;ITH&lt;/strong&gt;, or Intel Trace Hub, which is codenamed North Peak (NPK). The ITH can trace different internal hardware component (VIA - Visualization of Internal Signals, ODLA - On-chip logic analyzer, SoCHAP - SOC performance counters, IPT - Intel Process Trace, AET - Intel Architecture Trace), and external component like CSME, the UEFI firmware, and you can even connect it to ETW. This telemetry eventually finds its way to Intel in various methods.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/image2020-4-22_0-52-2.png&quot; alt=&quot;Intel Trace Hub&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;TPM&lt;/strong&gt; is designed to provide a tamper proof environment to enforce system security through hardware. It implements in hardware many essential functions: sha1 &amp;amp; sha256 hashing algorithms, many crypto and key derivation functions, measurment registers call the Platform Configuration Registers (PCRs), a secret key - Endorsment Key - used to derive all other keys, and non-volatile storage slots for storing keys and hashes. Discrete TPM chips (i.e. those that are a separate chip on the mainboard or SOC and connected through the LPC) are call dTPMs, or can be implemented in the CSME module's firmware and called fTPMs.&lt;/p&gt;

&lt;p&gt;The TPM's &lt;strong&gt;PCR&lt;/strong&gt; are initialized to zero when the platform boots and are filled up with measurements through the boot process. PCRs 0-15 are intended for &quot;static&quot; use - they reset when the platform boots; They are supposed to give the OS loader a view of the platform initialization state.
PCRs 17-22 are for &quot;dynamic&quot; use - they get reset on each secure launch (GETSEC[SENTER]); They are supposed to be used by the attestation sofware that checks if the OS is trusted.&lt;/p&gt;

&lt;h2 id=&quot;the-flash-chip&quot;&gt;The Flash Chip&lt;/h2&gt;
&lt;p&gt;SPI flash has 5 major regions: the Descriptor regions, the CSME region, the Gigabit Ethernet Region, the Platform Data Region, and the UEFI region. In the image below you can see an example of how the flash is organized.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/image2019-11-11_1-18-53_1.png&quot; alt=&quot;Partition regions in SPI flash&quot; /&gt;
&lt;img src=&quot;/images/image2019-11-11_1-19-12.png&quot; alt=&quot;Serial flash sizes&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Later versions added more regions:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/image2019-11-25_22-49-41.png&quot; alt=&quot;SPI region evolution&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These regions are categorized as fault tolerant (FTPs) and non fault tolerant partitions (NFTPs).
Fault tolerant partitions are critical for boot, and verified during early boot (like the RBE, the CSME ROM Boot extensions will discuss in a few paragraphs). If verification fails - the system does not boot. Examples of non fault tolerant partitions are the Integrated Sensor Hub (or ISH) firmware.&lt;/p&gt;

&lt;p&gt;SPI flash protection is applied at multiple levels: On the flash chip itself, in the SPI flash controller (in the PCH), in UEFI code and in CSME code.&lt;/p&gt;

&lt;p&gt;The SPI controller maps the entire flash to memory at a fixed address, so reads/writes are usually done simply by reading/writing memory. The SPI controller translates this to flash-specific commands issued on the SPI bus, using a table of flash-specific commands stored in the flash descriptor region.
This is called &quot;Hardware Sequencing&quot;, meaning the SPI controller issues the actual SPI commands
When hardware sequencing is in use, the SPI controller enforces several flash protections based on the masters region table in the flash (but can be overriden using a hardware PIN).&lt;/p&gt;

&lt;p&gt;The SPI controller also implements a FLOCKDN flag. FLOCKDN is a write-once bit that, when set, disables use of software sequencing and modification of the PR registers until the next reset. The CSME sets this in the Bring-UP process (bup_storage_lock_spi_configuration(), see below). This happens when the UEFI notifies it that it is at the end of POST. In addition to the region access control table, the SPI controller also has an option to globally protect up to five regions in the flash from write access by the host using five registers, called Protected Registers (PRs), which are intended for the UEFI firmware to protect itself from modification while the OS is running.&lt;/p&gt;

&lt;p&gt;It is also possible to issue direct flash commands using &quot;Software Sequencing&quot; by writing to the OPTYPE/OPMENU registers, since this can be used circumvent the SPI-enforced protections, software sequencing is usually disabled after POST using the FLOCKDN bit.&lt;/p&gt;

&lt;p&gt;How is the flash updated?&lt;/p&gt;

&lt;p&gt;UEFI region is updated through an UEFI capsule, This update happens during POST, before PRs and FLOCKDN is set, therefore, the BIOS region is still accessible to UEFI code.&lt;/p&gt;

&lt;p&gt;Many OEMS have then own UEFI anti-tamper protections. For example, HP has SureStart on laptops and workstations, and Dell has TrustedDevice SafeBIOS. SafeBIOS copies bad firmware images to the EFI system partition, and the Dell Trusted Device software on Windows sends their hashes plus the hash of the UEFI firmware currently in memory to a Dell cloud server (*.delltrusteddevicesecurity.com) to check against a list of &quot;authorized&quot; hashes. Server platforms have similiar protections, including iLO for HP and iDRAC in Dell.
The CSME region can usually be updated only from within the CSME. However, for more complicated upgrades CSME can temporarily unlock the ME region for host read &amp;amp; write.&lt;/p&gt;

&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;
&lt;p&gt;In the next sections we'll look over all the stages of boot.
&lt;img src=&quot;/images/image2020-4-20_14-56-23.png&quot; alt=&quot;Serial flash sizes&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;early-power-on&quot;&gt;Early power on&lt;/h1&gt;
&lt;p&gt;Boot starts the PMC, the Power Management Controller.
In modern Intel systems the PMC is an ARC core and its the first controller to execute code once electricity is applied to the system. We'll talk more about PMC in a later post as its quiet interesting and has its own microcode and firmware, and event generates telemetry over the IOSF SB bus (which we'll talk about in a moment).&lt;/p&gt;

&lt;p&gt;While the PMC does its init, the rest of the system is held at bay at a RESET state.&lt;/p&gt;

&lt;p&gt;The next part to start running is the CSME. Recall from the first post in the series, CSME, or Converged Security and Managment Engine is a MinuteIA (i486 CPU IP block) embedded in the Platform Controller Hub (PCH).
The CSME begins running from its own embedded 128KB ROM - the CSME-ROM. This ROM is protected with a hardware fuse that is burned by Intel during production.
When started the CSME ROM starts like a regular 486 processor BIOS - in the reset vector in real mode. Its first order of business is to enable protected mode. Next it checks if the system is configured in ROM bypass mode to assist debugging, if so maps the ROMB partition in SPI and starts executing from there - a mode call ROM bypass mode which we might dig into later.
Next the CSME's SRAM is initialized and a page table is created mapping SRAM and ROM and then paging is enabled. Once basic initialization is out of the way CSME can switch to C code that does some more complex initialization: initiating the IOMMU (AUnit), the IACP and hardware crypto keys which are calculated from fixed values in hardware. Finally, the DMA engine is used to read the next stage, called the Rom Boot Extension, or RBE, from the system firmware flash through SPI, and verifies it against the cryptographic keys prepared earlier. CSME ROM uses a special table, the Firmware Interface Table, or FIT, a table of pointers to specific regions in the flash and is itself stored in a fixed flash address.&lt;/p&gt;

&lt;p&gt;The RBE's job is to load the CSME OS kernel and verify it cryptographically. This process is optimized by using a mechanism called ICV, or Integrity-Check Values, hardware cached verified hashes - as long as the CSME kernel has the same hash it does not require crypto verification. Another check performed by the RBE is an anti-rollback check, making sure that once the CSME has been upgraded to a new version it cannot be downgraded back to the original version.
Before starting the main CSME kernel the RBE loads pre-OS modules. An example pre-OS module is IDLM, which can be used to load debug-signed firmware on a production platform.&lt;/p&gt;

&lt;p&gt;The kernel starts by enabling several platform security features: SMEP, Supervisor Mode Access Prevention, prevents exploits from running mapped kernel memory from ring3, and DEP, Data Execution Prevention, which prevents exploits from running code from stack regions. It also generates per-process syscall table permissions, aswell as ACL and IPC permissions.&lt;/p&gt;

&lt;h1 id=&quot;bring-up-bup&quot;&gt;Bring-Up (BUP)&lt;/h1&gt;

&lt;p&gt;Once everything is ready the kernel loads the Process Manager which executed &quot;IBL processes&quot;, which includes Bring-Up (BUP) and the Loader. The BUP loads virtual file system, or VFS server, parses the init script of the FTPR partition and loads all IBL modules listed there. This includes: the Event Dispatcher Server (eventdisp) - service that allows publishing, registering and acknowledging receipt of named events (sort of DBUS), the Bus Driver (busdrv) - a driver that permits other drivers to access devices on the CSME's internal bus, the RTC driver (prtc), the Crypto/DMA driver (crypto) - provices access to services offered by the OCS hardware (SKS, DMA engines), the Storage driver (storage) - which provides access to the MFS filesystem, the Fuse driver (fpf) and finally the Loader Server (loadmgr).&lt;/p&gt;

&lt;p&gt;As seen in the image below, this is the stage where the CPU finally begins execution.&lt;/p&gt;

&lt;h1 id=&quot;cpu-initialization&quot;&gt;CPU initialization&lt;/h1&gt;
&lt;p&gt;Once the CSME is ready it releases the main CPU from the RESET state. The main CPU loads microcode from the FIT table and sets it up (after CSME verified the uCode cryptographically) . I won't go into details about microcode, also called uCode, here as I have a full post planned on microcode later. Whats important to know is that microcode does not only include the &quot;implementation&quot; of the instruction set architecture (ISA), but also many routines for intilization, reset, paging, MSRs and much mich more.
As part of CPU initialization it loads another module from the FIT, the Authenticated Code Module (ACM).
The ACM implements BootGuard, a security feature to check cryptographically verify the UEFI signature before it is loaded (once called &quot;AnchorCove&quot;).
This begins the Static Root Of Trust Model (SRTM), where CSME ROM verifies the CSME, which verifies the microcode, which verifies the ACM, which verifies the UEFI firmware, which verifies the operating system. This is done by chaining their hashes and storing them in the TPM.
The ACM also initializes TXT, the Dynamic Root of Trust Model (DRTM) which we will detail in a few paragraphs.&lt;/p&gt;

&lt;h1 id=&quot;uefi-initialization&quot;&gt;UEFI initialization&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;/images/image2019-11-20_15-11-32.png&quot; alt=&quot;UEFI Initialization stages&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once the CPU completes initialization, the Initial Boot Block (IBB) of the UEFI firmware is executed.
The startup ACM authenticates parts of the FIT and the IBB using the OEM key burned into the fuses, authenticates it and measures it into PCR0 in the TPM. PCR0 is also referred to as the CRTM (Core Root of Trust Measurement)&lt;/p&gt;

&lt;p&gt;The first stage of IBB is SEC which is responsible for very early platform initialisation, and loading the UEFI secure boot databases from non-volatile (NV) storage (these keys have various names such as PK, KEK, DB, DBX). Next comes PEI core, or &quot;main&quot; module of the Pre EFI initialization. It loads several modules (PEIMs) that initialiaze basic hardware such as memory, PCI-E, USB, basic graphics, basic power managment and more. Some of this code is implemented by the UEFI vendors or OEMs, and some come from Intel in &quot;FSPs&quot;, Firmware Support Packages, which perform &quot;Silicon Initialization&quot;. Common UEFI firmwears can have as many as a 100 PIE modules.&lt;/p&gt;

&lt;p&gt;The UEFI spec does not covers signature/authentication checks in PEI phases. Thats why Intel needed BootGuard to do the bootstrapping: At power-on, BootGuard measures the IBB ranges which include PEI.&lt;/p&gt;

&lt;p&gt;Following PEI the Driver Execution Environment is loaded by a security PEI module which verifies their integrity cryptographically beforehand. DXE is responsible for setting up all the rest of the hardware and software execution environment in preparation for OS loading. It also setups System Management Mode (which we'll talk about soon), sensors and monitoring, boot services, real-time clocks and more. A modern UEFI firmware can have as much as 200 different DXE drivers installed.&lt;/p&gt;

&lt;p&gt;Many OEMs use BootGuard to authenticate DXE as well by configuring the IBBs to include the entire PEI volume in the flash (PEI Core + PEI modules) and the DXE Core. Secure Boot is used to verify each PEI/DXE image that is loaded before executing it. These images are measured and extended into the TPM's PCR0 as well.&lt;/p&gt;

&lt;p&gt;The DXE environment initializes two important tables: the EFI Runtime services table and the EFI Boot Service Table.
Boot Services are used by the operating system only during boot and discarded thereafter. These include memory allocation services and services to access DXE drivers like storage, networking and display. Runtime services are kept in memory for use by the operating system whenever required, and include routines for getting and setting the value of EFI variables, clock manipulation, hardware configuration, firmware capsule updates and more.&lt;/p&gt;

&lt;p&gt;Finally the UEFI firmware measures the platform (e.g. chipset) security configuration (NV variables) into PCR1 and then locks them by calling a function in the ACM.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/image2019-11-27_12-24-21.png&quot; alt=&quot;UEFI boot stages&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;loading-the-boot-loader&quot;&gt;Loading the boot loader&lt;/h2&gt;
&lt;p&gt;The final driver to be loaded by DXE is the Bood Device Selection module or BDS. BDS scans its stored configuration, comparing it with the currently available hardware and decides on a boot device. This gets executed in legacy boot and non secureboot systems.
In SecureBoot mode another DXE component called the SecureBootDXE is loaded to authenticate the OS boot loader. The cryptographic key used is stored in DXE and verified as part of BootGuard. SecureBootDXE also compares the boot loader agains a signed list of blacklisted or whitelisted loaders.&lt;/p&gt;

&lt;h1 id=&quot;windows-boot&quot;&gt;Windows Boot&lt;/h1&gt;

&lt;p&gt;Now we are ready for Transient System Load (TSL), most of DXE gets discarded and the OS bootloader is loaded.
The bootloader (called the IPL) is measured into PCR4 and control is transfered to it.
For Windows this is bootmgrfw.efi, the Windows Boot Manager. It first initialzes security policies, handles sleep states like hibernation, and finally uses EFI boot services to load the Windos loader, winload.efi.&lt;/p&gt;

&lt;h2 id=&quot;winload&quot;&gt;Winload&lt;/h2&gt;
&lt;p&gt;Winload initializes the system's page tables in preparation for loading the kernel, loads the system registry hive, loads the Kernel and the Hardware Abstraction Layer (HAL DLL) and early boot drivers. They are all authenticated cryptographically, and their measurement are stored into the TPM. Once thats done, it uses UEFI memory services to initialze the IOMMU. Once everything is loaded into its correct place in memory, the EFI boot service are discarded.&lt;/p&gt;

&lt;h2 id=&quot;hvci&quot;&gt;HVCI&lt;/h2&gt;
&lt;p&gt;When HVCI, or HyperVisor protected Code Integrity is enabled a different process occurs. Winload does not load the kernel, instead loading the Hypervisor loader (hvload.efi), which in turn loads the hypervisor (hvix64.exe), and sets up a protected virtual machine called VTL1 - Virtual Trust Level 1. It then loads the Secure Kernel (SK) into VTL1, and then setups VTL0, the untrusted level for the normal kernel. Now winload.efi is resumed within VTL0 and continues to boot the system within VTL0. The secure kernel continues running in the background providing security features like authentication as well as memory protection services for VTL0.&lt;/p&gt;

&lt;p&gt;Its important to note that the hypervisor and secure kernel do not trust UEFI, and do not initiate any UEFI calls while running. Any future UEFI runtime service calls will be executed from within the VTL0 virtual machine thus protected from harming the hypervisor and secure kernel.&lt;/p&gt;

&lt;p&gt;The regular OS kernel boot then continues in VTL0. Malicous UEFI and driver code cannot affect the hypervisor or the secure kernel. Malicious drivers can and will continue to attack user mode code in VTL0, but they must be signed by Microsoft and thus can be analyzed before being approved or blocked quickly if a bug/exploit is found.&lt;/p&gt;

&lt;h2 id=&quot;dynamic-root-of-trust-model-drtm&quot;&gt;Dynamic Root of Trust Model (DRTM)&lt;/h2&gt;
&lt;p&gt;The whole security model presented so far is based on a chain of verifications. But what happens if that chain is broken by a bug? UEFI implementations have many security bugs, and those will affect the security of the whole system.
To alleviate this issue Intel and Microsoft developed the Dynamic Root of Trust Model (DRTM), available since Windows 10 18H2.
In DRTM, winload starts a new load verification chain using an Intel security feature called TXT. TXT measures critical parts of the OS during OS loading. The process is initiated by the OS executing a special instruction - GETSEC[SENTER], implemented in microcode, which results in the loading, authentication and execution of an ACM called the Secure Init ACM (SINIT ACM). The ACM can be on the flash, or can be supplied by the OS with the GETSEC instruction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/tpm.png&quot; alt=&quot;DRTM Model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The GETSEC-SENTER microcode flow clears PCR17-23, does an initial measurement into PCR17 that includes the SINIT ACM and the parameters of the GETSEC instruction and executes the SINIT ACM.
SINIT measures additional secure-launch related stuff into PCR17 which includes the STM (if present), digest of Intel Early TXT code and matching elements of the Launch Control Policy (LCP). The LCP checks the platform is in a known-good state by checking PCRs 0-7, and that the OS is in a known-good state by checking PCRs 18-19.
Next SINIT measures authorities involved up to now into PCR18 (the measurement is of the authority (e.g. the signer/key) and not the data to allow for upgrades).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/image2020-5-11_13-58-11.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The OS now continues to load and use the PCRs for attestation telemetry.&lt;/p&gt;

&lt;p&gt;SecureBoot + DRTM + BitLocker (Windows uses PCRs 7 and 11 for Secure Boot based BitLocker) make sure the system is almost impervious to attacks.&lt;/p&gt;

&lt;p&gt;The Windows secure boot process is implemented in an executable call tcblaunch.exe, TCB - Trusted Compute Base.
This is the executable the SINIT ACM measures and launches. The reason tcblaunch.exe was inevented is that data generated from within tcblaunch is considered secure, while data generated from winload can be tainted.
A funny artifact of the MLE launch process is caused by the fact that it is 32-bit, but tcblaunch.exe is 64-bit. Microsoft hacked this by providing a 32-bit mlestartup.'exe binary inside the MSDOS header region of the MZ/PE file.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/mle.png&quot; alt=&quot;Windows MLE + HV&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;uefi-memory-attributes-table&quot;&gt;UEFI Memory Attributes Table&lt;/h2&gt;
&lt;p&gt;As stated before, Windows wants to run the UEFI runtime services in VTL0. By default the OS cannot lock these memory pages to be W^X (only write or only execute, not both) because many old UEFI systems still mix code and data.
Microsoft solves this by introducing a new UEFI table, the UEFI Memory Attributes Table (MAT), which specifies if the runtime service should execute from VTL0 (by marking the memory region as EFI_MEMORY_RO|EFI_MEMORY_XP), or must run with RWX protections. Since this is a gaping whole, the UEFI runtime's parameters are santized using a VTL code - and this is enabled only for a restricted subset of runtime calls).&lt;/p&gt;

&lt;h1 id=&quot;other-oss&quot;&gt;Other OSs&lt;/h1&gt;
&lt;p&gt;[IMAGE]&lt;/p&gt;

&lt;p&gt;Some Linux distrubutions use Intel TBOOT implementation for DTRM launch. VMware ESXi support DRTM and TXT from version 6.7U1 using a customized version of TBOOT, and attastation information is managed through VSphere.&lt;/p&gt;

&lt;p&gt;[ IMAGE]&lt;/p&gt;

&lt;h1 id=&quot;more-protections&quot;&gt;More Protections&lt;/h1&gt;

&lt;h2 id=&quot;iommu-and-dma-protections&quot;&gt;IOMMU and DMA protections&lt;/h2&gt;
&lt;p&gt;DMA is a platform feature that allows hardware to write directly to main memory bypassing the CPU. This greatly enhances performance, but comes with a security cost: hardware can overwrite UEFI or OS memory after it has been measured and authenticated. This means malicous hardware can attack the OS after boot and tamper with it.&lt;/p&gt;

&lt;p&gt;To solve this problem the memory managment controller of the platform was extended to protect IO, and called the IOMMU. Intel calls this technology VT-d, and it implements address paging with permissions for DMA. The IOMMU allows the OS and its drivers to setup the memory regions devices are allowed to write to. Another protection mechanism in IOMMU used by the UEFI firmware and later the OS is Protected Memory Regions, or PMRs. These define regions that can only be accessed from the OS on the CPU and never by devices through DMA. The IOMMU must be enabled very quickly early in boot to protect from malicous on-board firmware attacking before the OS loads.&lt;/p&gt;

&lt;p&gt;To ensure the mechanism for setting up the PMRs is not tampered with it too is measured, including the IOMMU ACPI table, the APIC table, the RAM structure definition, and DMA protection information.&lt;/p&gt;

&lt;p&gt;Windows uses the IOMMU and PMRs to protect itself since Windows 10 18H2, and calls this feature Kernel DMA Protection. The Kernel DMA protection prevents DMA to VTL1, hypervisor and VTL0's kernel regions.
Microsoft also allows special implement&lt;/p&gt;

&lt;p&gt;There is an undocumented feature in the kernel used by Graphics/DirectX to allow sharing the kernel's virtual memory address space with the graphics card (Device-TLB, ExShareAddressSpaceWithDevice()).&lt;/p&gt;

&lt;h2 id=&quot;secure-devices&quot;&gt;Secure Devices&lt;/h2&gt;
&lt;p&gt;Microsoft allows some device to be isolated from VTL0 and used only from code in VTL1 to protect sensitive information used for logon, like the face recognition camera and fingerprint sensors. Secure devices discovered using ACPI table &quot;SDEV&quot; (SDEV_SECURE_RESOURCE_ID_ENTRY, SDEV_SECURE_RESOURCE_MEMORY_ENTRY).&lt;/p&gt;

&lt;p&gt;Secure devices can be either pure-ACPI devices or PCI devices. Both can be targets for DMA requests&lt;/p&gt;

&lt;p&gt;It seems the drivers for secure devices are actually VTL1 user-mode processes that call basic functions in IUMBASE to communicate with the device (DMA, read/write PCI configuration space, do memory-mapped IO), for example:
GetDmaEnabler / DmaMapMemory / SetDmaTargetProperties / MapSecureIo / UnmapSecureIo&lt;/p&gt;

&lt;h2 id=&quot;smm&quot;&gt;SMM&lt;/h2&gt;
&lt;p&gt;SMM, or the System Managment Mode, is a special mode invoked to handle various hardware and software interrupts, and is implemented as part of the UEFI firmware.
For example, SMM can simulate a PS/2 keyboard by handling keyboard interrupts and translating them into USB read/write. When a legacy application performs an IO IN/OUT operation on a PS/2 port, the SMI handler registered for that port is executed, transfers the system into SMM mode, runs the DXE USB keyboard driver, and then returns the result transparently.
SMM is also used for security features by allowing certain actions to occur only from SMM.
The caveat of SMM is that it has full access to the system, and operates in &quot;ring -2&quot;, even higher then VTL-1 and the hypervisor. It has been used for attacks for many years (look in google for NSA's SOUFFLETROUGH).&lt;/p&gt;

&lt;p&gt;Intel &amp;amp; Microsoft have developed three technologies to protect the OS from SMM: IRBR, STM, PPAM.&lt;/p&gt;

&lt;p&gt;IRBR, or Intel Runtime BIOS Resilience, runs the SMI handler in protected mode with paging enabled, with a page table set up to only map SMRAM, as well as CPU protection to prevent changes to the paging table in SMM mode.&lt;/p&gt;

&lt;p&gt;STM - SMM Transfer Monitor, means that most of the SMI handler virtualized, with only a small part called the STM serving as its hypervisor. I don't think is actually implemented in UEFI.&lt;/p&gt;

&lt;p&gt;PPAM - also called Nifty Rock or Devil's Gate Rock, tries to fill the gap between IRBR and STM by prepending an Intel entry-point to the SMI handler. Intel supplies a signed module called PPAM that can measure certain attributes of the SMI handler and report them to the OS. The OS can then make a policy decision on how to proceed.
All SMI handler must also be registered in a table called the WSMT table. The firmware's WSMT tables declares to the OS that the firmware guarantees three things: FixedCommBuffers - a guarantee that the SMM will vaildate that the input/output buffers of the operation, CommBufferNestedPtrProtection that extends this guarantee to any pointers within input/output structures, and SystemResourceProtection that indicated that the SMI handler will not reconfigure the hardware.&lt;/p&gt;

&lt;h2 id=&quot;memory-reset-protections&quot;&gt;Memory Reset protections&lt;/h2&gt;
&lt;p&gt;After a warm boot or even a fast cold boot some secrets (keys) might remain in memory. Intel provides security for these secrets using special TXT Secrets registers.&lt;/p&gt;</content><author><name></name></author><summary type="html">Introduction and System Architecture Buses More Components The Flash Chip Overview Early power on Bring-Up (BUP) CPU initialization UEFI initialization Loading the boot loader Windows Boot Winload HVCI Dynamic Root of Trust Model (DRTM) UEFI Memory Attributes Table Other OSs More Protections IOMMU and DMA protections Secure Devices SMM Memory Reset protections This blog post is an in-depth dive into the security features of the Intel/Windows platform boot process. In this post I'll explain the startup process through security focused lenses, next post we'll dive into several known attacks and how they were handled by Intel and Microsoft. My wish is to explain to technology professionals not deep into platform security why Microsoft's SecureCore is so important and necessary. Introduction and System Architecture We must first begin with a brief introduction to the hardware platform. Skip this if you have read the awsome material available on the web about the Intel architecture, I'll try to briefly summarize it here. The Intel platform is based on one or two chips. Small systems have one, the desktop and server ones are separated to a CPU complex and a PCH complex (PCH = Platform Controller Hub). The CPU complex deals with computation. It holds the &quot;processor&quot; cores, e.g. Sunny Cove that implement the ISA, as well as cross core caches like the L3 cache, and more controllers that are grouped together as &quot;the system agent&quot; or the &quot;uncore&quot;. The uncore contains the memory controller and display, e.g. GPU and display controller. The PCH handles all other IO, including access to the firmware through SPI or eSPI, wifi, LAN, USB, HD audio, SMBus, thunderbolt and etc'. The PCH also hosts several embedded processors, like the PMC, the Power Management Controller. An additional part of the PCH is a very important player in our story, the CSME, or Converged Security &amp;amp; Management Engine, a i486 IP block (also called Minute IA). CSME is responsibly for much of the security model of Intel processors as well as many of the manageability features of the platform. The CSME block has its own dedicated ~1.5mb of SRAM memory and 128KB of ROM, as well as a dedicated IOMMU, called the A-Unit (that even has its own acode microcode) located in the CSME's uncore', thats allows access from ME to the main memory, as well as DMA to/from the main memory and using the main memory as an encrypted paging area (&quot;virtual memory&quot;). The CSME engine runs a customized version of the Minix3 microkernel, also recent versions have changed it beyond recognition adding many security features. Buses Lets use this post to also introduce the main interconnects in the system. The main externally facing interconnect bus is PCI-E, a fast bust that can reach 64GBps in its latest incarnations. A second external bus is the LPC, or Low Pin Count bus, a slow bus for connecting devices such as SPI flash, the TPM (explained below), and old peripherals such as PS/2 touchpads. Internally the platform is based around the IOSF, or Intel On-chip System Fabric, which is a pumped up version of PCI-E that supports many additional security and addressing features. For addressing IOSF adds SourceID and DestID fields that contain the source and destination of any IOSF transaction, extending PCI-E Bus-Device-Function (BDF) addressing to enable routing over bridges. IOSF also extends addressing by adding support for multiple address root namespaces, currently defining three: RS0 for host memory space, RS1 for CSME memory space, and RS2 for the Innovation-Engine (IE), another embedded controller currently present only on server chipsets. There are two IOSF busses in the PCH - the Primary Fabric and the Sideband Fabric. The Primary Fabric is high speed, connecting the CPU to the PCH (through a protocol call DMI), as well as high speed devices such as Gigbait Ethernet, WiFi and eSPI. The Sideband Fabric is used to connect the CSME to low-speed devices, including the PMC (Power Management Controller), the RNG generator, GPIO pins, USB, SMBus, and even debugging interfaces such as JTAG. More Components Another interesting component is the ITH, or Intel Trace Hub, which is codenamed North Peak (NPK). The ITH can trace different internal hardware component (VIA - Visualization of Internal Signals, ODLA - On-chip logic analyzer, SoCHAP - SOC performance counters, IPT - Intel Process Trace, AET - Intel Architecture Trace), and external component like CSME, the UEFI firmware, and you can even connect it to ETW. This telemetry eventually finds its way to Intel in various methods. The TPM is designed to provide a tamper proof environment to enforce system security through hardware. It implements in hardware many essential functions: sha1 &amp;amp; sha256 hashing algorithms, many crypto and key derivation functions, measurment registers call the Platform Configuration Registers (PCRs), a secret key - Endorsment Key - used to derive all other keys, and non-volatile storage slots for storing keys and hashes. Discrete TPM chips (i.e. those that are a separate chip on the mainboard or SOC and connected through the LPC) are call dTPMs, or can be implemented in the CSME module's firmware and called fTPMs. The TPM's PCR are initialized to zero when the platform boots and are filled up with measurements through the boot process. PCRs 0-15 are intended for &quot;static&quot; use - they reset when the platform boots; They are supposed to give the OS loader a view of the platform initialization state. PCRs 17-22 are for &quot;dynamic&quot; use - they get reset on each secure launch (GETSEC[SENTER]); They are supposed to be used by the attestation sofware that checks if the OS is trusted. The Flash Chip SPI flash has 5 major regions: the Descriptor regions, the CSME region, the Gigabit Ethernet Region, the Platform Data Region, and the UEFI region. In the image below you can see an example of how the flash is organized. Later versions added more regions: These regions are categorized as fault tolerant (FTPs) and non fault tolerant partitions (NFTPs). Fault tolerant partitions are critical for boot, and verified during early boot (like the RBE, the CSME ROM Boot extensions will discuss in a few paragraphs). If verification fails - the system does not boot. Examples of non fault tolerant partitions are the Integrated Sensor Hub (or ISH) firmware. SPI flash protection is applied at multiple levels: On the flash chip itself, in the SPI flash controller (in the PCH), in UEFI code and in CSME code. The SPI controller maps the entire flash to memory at a fixed address, so reads/writes are usually done simply by reading/writing memory. The SPI controller translates this to flash-specific commands issued on the SPI bus, using a table of flash-specific commands stored in the flash descriptor region. This is called &quot;Hardware Sequencing&quot;, meaning the SPI controller issues the actual SPI commands When hardware sequencing is in use, the SPI controller enforces several flash protections based on the masters region table in the flash (but can be overriden using a hardware PIN). The SPI controller also implements a FLOCKDN flag. FLOCKDN is a write-once bit that, when set, disables use of software sequencing and modification of the PR registers until the next reset. The CSME sets this in the Bring-UP process (bup_storage_lock_spi_configuration(), see below). This happens when the UEFI notifies it that it is at the end of POST. In addition to the region access control table, the SPI controller also has an option to globally protect up to five regions in the flash from write access by the host using five registers, called Protected Registers (PRs), which are intended for the UEFI firmware to protect itself from modification while the OS is running. It is also possible to issue direct flash commands using &quot;Software Sequencing&quot; by writing to the OPTYPE/OPMENU registers, since this can be used circumvent the SPI-enforced protections, software sequencing is usually disabled after POST using the FLOCKDN bit. How is the flash updated? UEFI region is updated through an UEFI capsule, This update happens during POST, before PRs and FLOCKDN is set, therefore, the BIOS region is still accessible to UEFI code. Many OEMS have then own UEFI anti-tamper protections. For example, HP has SureStart on laptops and workstations, and Dell has TrustedDevice SafeBIOS. SafeBIOS copies bad firmware images to the EFI system partition, and the Dell Trusted Device software on Windows sends their hashes plus the hash of the UEFI firmware currently in memory to a Dell cloud server (*.delltrusteddevicesecurity.com) to check against a list of &quot;authorized&quot; hashes. Server platforms have similiar protections, including iLO for HP and iDRAC in Dell. The CSME region can usually be updated only from within the CSME. However, for more complicated upgrades CSME can temporarily unlock the ME region for host read &amp;amp; write. Overview In the next sections we'll look over all the stages of boot. Early power on Boot starts the PMC, the Power Management Controller. In modern Intel systems the PMC is an ARC core and its the first controller to execute code once electricity is applied to the system. We'll talk more about PMC in a later post as its quiet interesting and has its own microcode and firmware, and event generates telemetry over the IOSF SB bus (which we'll talk about in a moment). While the PMC does its init, the rest of the system is held at bay at a RESET state. The next part to start running is the CSME. Recall from the first post in the series, CSME, or Converged Security and Managment Engine is a MinuteIA (i486 CPU IP block) embedded in the Platform Controller Hub (PCH). The CSME begins running from its own embedded 128KB ROM - the CSME-ROM. This ROM is protected with a hardware fuse that is burned by Intel during production. When started the CSME ROM starts like a regular 486 processor BIOS - in the reset vector in real mode. Its first order of business is to enable protected mode. Next it checks if the system is configured in ROM bypass mode to assist debugging, if so maps the ROMB partition in SPI and starts executing from there - a mode call ROM bypass mode which we might dig into later. Next the CSME's SRAM is initialized and a page table is created mapping SRAM and ROM and then paging is enabled. Once basic initialization is out of the way CSME can switch to C code that does some more complex initialization: initiating the IOMMU (AUnit), the IACP and hardware crypto keys which are calculated from fixed values in hardware. Finally, the DMA engine is used to read the next stage, called the Rom Boot Extension, or RBE, from the system firmware flash through SPI, and verifies it against the cryptographic keys prepared earlier. CSME ROM uses a special table, the Firmware Interface Table, or FIT, a table of pointers to specific regions in the flash and is itself stored in a fixed flash address. The RBE's job is to load the CSME OS kernel and verify it cryptographically. This process is optimized by using a mechanism called ICV, or Integrity-Check Values, hardware cached verified hashes - as long as the CSME kernel has the same hash it does not require crypto verification. Another check performed by the RBE is an anti-rollback check, making sure that once the CSME has been upgraded to a new version it cannot be downgraded back to the original version. Before starting the main CSME kernel the RBE loads pre-OS modules. An example pre-OS module is IDLM, which can be used to load debug-signed firmware on a production platform. The kernel starts by enabling several platform security features: SMEP, Supervisor Mode Access Prevention, prevents exploits from running mapped kernel memory from ring3, and DEP, Data Execution Prevention, which prevents exploits from running code from stack regions. It also generates per-process syscall table permissions, aswell as ACL and IPC permissions. Bring-Up (BUP) Once everything is ready the kernel loads the Process Manager which executed &quot;IBL processes&quot;, which includes Bring-Up (BUP) and the Loader. The BUP loads virtual file system, or VFS server, parses the init script of the FTPR partition and loads all IBL modules listed there. This includes: the Event Dispatcher Server (eventdisp) - service that allows publishing, registering and acknowledging receipt of named events (sort of DBUS), the Bus Driver (busdrv) - a driver that permits other drivers to access devices on the CSME's internal bus, the RTC driver (prtc), the Crypto/DMA driver (crypto) - provices access to services offered by the OCS hardware (SKS, DMA engines), the Storage driver (storage) - which provides access to the MFS filesystem, the Fuse driver (fpf) and finally the Loader Server (loadmgr). As seen in the image below, this is the stage where the CPU finally begins execution. CPU initialization Once the CSME is ready it releases the main CPU from the RESET state. The main CPU loads microcode from the FIT table and sets it up (after CSME verified the uCode cryptographically) . I won't go into details about microcode, also called uCode, here as I have a full post planned on microcode later. Whats important to know is that microcode does not only include the &quot;implementation&quot; of the instruction set architecture (ISA), but also many routines for intilization, reset, paging, MSRs and much mich more. As part of CPU initialization it loads another module from the FIT, the Authenticated Code Module (ACM). The ACM implements BootGuard, a security feature to check cryptographically verify the UEFI signature before it is loaded (once called &quot;AnchorCove&quot;). This begins the Static Root Of Trust Model (SRTM), where CSME ROM verifies the CSME, which verifies the microcode, which verifies the ACM, which verifies the UEFI firmware, which verifies the operating system. This is done by chaining their hashes and storing them in the TPM. The ACM also initializes TXT, the Dynamic Root of Trust Model (DRTM) which we will detail in a few paragraphs. UEFI initialization Once the CPU completes initialization, the Initial Boot Block (IBB) of the UEFI firmware is executed. The startup ACM authenticates parts of the FIT and the IBB using the OEM key burned into the fuses, authenticates it and measures it into PCR0 in the TPM. PCR0 is also referred to as the CRTM (Core Root of Trust Measurement) The first stage of IBB is SEC which is responsible for very early platform initialisation, and loading the UEFI secure boot databases from non-volatile (NV) storage (these keys have various names such as PK, KEK, DB, DBX). Next comes PEI core, or &quot;main&quot; module of the Pre EFI initialization. It loads several modules (PEIMs) that initialiaze basic hardware such as memory, PCI-E, USB, basic graphics, basic power managment and more. Some of this code is implemented by the UEFI vendors or OEMs, and some come from Intel in &quot;FSPs&quot;, Firmware Support Packages, which perform &quot;Silicon Initialization&quot;. Common UEFI firmwears can have as many as a 100 PIE modules. The UEFI spec does not covers signature/authentication checks in PEI phases. Thats why Intel needed BootGuard to do the bootstrapping: At power-on, BootGuard measures the IBB ranges which include PEI. Following PEI the Driver Execution Environment is loaded by a security PEI module which verifies their integrity cryptographically beforehand. DXE is responsible for setting up all the rest of the hardware and software execution environment in preparation for OS loading. It also setups System Management Mode (which we'll talk about soon), sensors and monitoring, boot services, real-time clocks and more. A modern UEFI firmware can have as much as 200 different DXE drivers installed. Many OEMs use BootGuard to authenticate DXE as well by configuring the IBBs to include the entire PEI volume in the flash (PEI Core + PEI modules) and the DXE Core. Secure Boot is used to verify each PEI/DXE image that is loaded before executing it. These images are measured and extended into the TPM's PCR0 as well. The DXE environment initializes two important tables: the EFI Runtime services table and the EFI Boot Service Table. Boot Services are used by the operating system only during boot and discarded thereafter. These include memory allocation services and services to access DXE drivers like storage, networking and display. Runtime services are kept in memory for use by the operating system whenever required, and include routines for getting and setting the value of EFI variables, clock manipulation, hardware configuration, firmware capsule updates and more. Finally the UEFI firmware measures the platform (e.g. chipset) security configuration (NV variables) into PCR1 and then locks them by calling a function in the ACM. Loading the boot loader The final driver to be loaded by DXE is the Bood Device Selection module or BDS. BDS scans its stored configuration, comparing it with the currently available hardware and decides on a boot device. This gets executed in legacy boot and non secureboot systems. In SecureBoot mode another DXE component called the SecureBootDXE is loaded to authenticate the OS boot loader. The cryptographic key used is stored in DXE and verified as part of BootGuard. SecureBootDXE also compares the boot loader agains a signed list of blacklisted or whitelisted loaders. Windows Boot Now we are ready for Transient System Load (TSL), most of DXE gets discarded and the OS bootloader is loaded. The bootloader (called the IPL) is measured into PCR4 and control is transfered to it. For Windows this is bootmgrfw.efi, the Windows Boot Manager. It first initialzes security policies, handles sleep states like hibernation, and finally uses EFI boot services to load the Windos loader, winload.efi. Winload Winload initializes the system's page tables in preparation for loading the kernel, loads the system registry hive, loads the Kernel and the Hardware Abstraction Layer (HAL DLL) and early boot drivers. They are all authenticated cryptographically, and their measurement are stored into the TPM. Once thats done, it uses UEFI memory services to initialze the IOMMU. Once everything is loaded into its correct place in memory, the EFI boot service are discarded. HVCI When HVCI, or HyperVisor protected Code Integrity is enabled a different process occurs. Winload does not load the kernel, instead loading the Hypervisor loader (hvload.efi), which in turn loads the hypervisor (hvix64.exe), and sets up a protected virtual machine called VTL1 - Virtual Trust Level 1. It then loads the Secure Kernel (SK) into VTL1, and then setups VTL0, the untrusted level for the normal kernel. Now winload.efi is resumed within VTL0 and continues to boot the system within VTL0. The secure kernel continues running in the background providing security features like authentication as well as memory protection services for VTL0. Its important to note that the hypervisor and secure kernel do not trust UEFI, and do not initiate any UEFI calls while running. Any future UEFI runtime service calls will be executed from within the VTL0 virtual machine thus protected from harming the hypervisor and secure kernel. The regular OS kernel boot then continues in VTL0. Malicous UEFI and driver code cannot affect the hypervisor or the secure kernel. Malicious drivers can and will continue to attack user mode code in VTL0, but they must be signed by Microsoft and thus can be analyzed before being approved or blocked quickly if a bug/exploit is found. Dynamic Root of Trust Model (DRTM) The whole security model presented so far is based on a chain of verifications. But what happens if that chain is broken by a bug? UEFI implementations have many security bugs, and those will affect the security of the whole system. To alleviate this issue Intel and Microsoft developed the Dynamic Root of Trust Model (DRTM), available since Windows 10 18H2. In DRTM, winload starts a new load verification chain using an Intel security feature called TXT. TXT measures critical parts of the OS during OS loading. The process is initiated by the OS executing a special instruction - GETSEC[SENTER], implemented in microcode, which results in the loading, authentication and execution of an ACM called the Secure Init ACM (SINIT ACM). The ACM can be on the flash, or can be supplied by the OS with the GETSEC instruction. The GETSEC-SENTER microcode flow clears PCR17-23, does an initial measurement into PCR17 that includes the SINIT ACM and the parameters of the GETSEC instruction and executes the SINIT ACM. SINIT measures additional secure-launch related stuff into PCR17 which includes the STM (if present), digest of Intel Early TXT code and matching elements of the Launch Control Policy (LCP). The LCP checks the platform is in a known-good state by checking PCRs 0-7, and that the OS is in a known-good state by checking PCRs 18-19. Next SINIT measures authorities involved up to now into PCR18 (the measurement is of the authority (e.g. the signer/key) and not the data to allow for upgrades). The OS now continues to load and use the PCRs for attestation telemetry. SecureBoot + DRTM + BitLocker (Windows uses PCRs 7 and 11 for Secure Boot based BitLocker) make sure the system is almost impervious to attacks. The Windows secure boot process is implemented in an executable call tcblaunch.exe, TCB - Trusted Compute Base. This is the executable the SINIT ACM measures and launches. The reason tcblaunch.exe was inevented is that data generated from within tcblaunch is considered secure, while data generated from winload can be tainted. A funny artifact of the MLE launch process is caused by the fact that it is 32-bit, but tcblaunch.exe is 64-bit. Microsoft hacked this by providing a 32-bit mlestartup.'exe binary inside the MSDOS header region of the MZ/PE file. UEFI Memory Attributes Table As stated before, Windows wants to run the UEFI runtime services in VTL0. By default the OS cannot lock these memory pages to be W^X (only write or only execute, not both) because many old UEFI systems still mix code and data. Microsoft solves this by introducing a new UEFI table, the UEFI Memory Attributes Table (MAT), which specifies if the runtime service should execute from VTL0 (by marking the memory region as EFI_MEMORY_RO|EFI_MEMORY_XP), or must run with RWX protections. Since this is a gaping whole, the UEFI runtime's parameters are santized using a VTL code - and this is enabled only for a restricted subset of runtime calls). Other OSs [IMAGE] Some Linux distrubutions use Intel TBOOT implementation for DTRM launch. VMware ESXi support DRTM and TXT from version 6.7U1 using a customized version of TBOOT, and attastation information is managed through VSphere. [ IMAGE] More Protections IOMMU and DMA protections DMA is a platform feature that allows hardware to write directly to main memory bypassing the CPU. This greatly enhances performance, but comes with a security cost: hardware can overwrite UEFI or OS memory after it has been measured and authenticated. This means malicous hardware can attack the OS after boot and tamper with it. To solve this problem the memory managment controller of the platform was extended to protect IO, and called the IOMMU. Intel calls this technology VT-d, and it implements address paging with permissions for DMA. The IOMMU allows the OS and its drivers to setup the memory regions devices are allowed to write to. Another protection mechanism in IOMMU used by the UEFI firmware and later the OS is Protected Memory Regions, or PMRs. These define regions that can only be accessed from the OS on the CPU and never by devices through DMA. The IOMMU must be enabled very quickly early in boot to protect from malicous on-board firmware attacking before the OS loads. To ensure the mechanism for setting up the PMRs is not tampered with it too is measured, including the IOMMU ACPI table, the APIC table, the RAM structure definition, and DMA protection information. Windows uses the IOMMU and PMRs to protect itself since Windows 10 18H2, and calls this feature Kernel DMA Protection. The Kernel DMA protection prevents DMA to VTL1, hypervisor and VTL0's kernel regions. Microsoft also allows special implement There is an undocumented feature in the kernel used by Graphics/DirectX to allow sharing the kernel's virtual memory address space with the graphics card (Device-TLB, ExShareAddressSpaceWithDevice()). Secure Devices Microsoft allows some device to be isolated from VTL0 and used only from code in VTL1 to protect sensitive information used for logon, like the face recognition camera and fingerprint sensors. Secure devices discovered using ACPI table &quot;SDEV&quot; (SDEV_SECURE_RESOURCE_ID_ENTRY, SDEV_SECURE_RESOURCE_MEMORY_ENTRY). Secure devices can be either pure-ACPI devices or PCI devices. Both can be targets for DMA requests It seems the drivers for secure devices are actually VTL1 user-mode processes that call basic functions in IUMBASE to communicate with the device (DMA, read/write PCI configuration space, do memory-mapped IO), for example: GetDmaEnabler / DmaMapMemory / SetDmaTargetProperties / MapSecureIo / UnmapSecureIo SMM SMM, or the System Managment Mode, is a special mode invoked to handle various hardware and software interrupts, and is implemented as part of the UEFI firmware. For example, SMM can simulate a PS/2 keyboard by handling keyboard interrupts and translating them into USB read/write. When a legacy application performs an IO IN/OUT operation on a PS/2 port, the SMI handler registered for that port is executed, transfers the system into SMM mode, runs the DXE USB keyboard driver, and then returns the result transparently. SMM is also used for security features by allowing certain actions to occur only from SMM. The caveat of SMM is that it has full access to the system, and operates in &quot;ring -2&quot;, even higher then VTL-1 and the hypervisor. It has been used for attacks for many years (look in google for NSA's SOUFFLETROUGH). Intel &amp;amp; Microsoft have developed three technologies to protect the OS from SMM: IRBR, STM, PPAM. IRBR, or Intel Runtime BIOS Resilience, runs the SMI handler in protected mode with paging enabled, with a page table set up to only map SMRAM, as well as CPU protection to prevent changes to the paging table in SMM mode. STM - SMM Transfer Monitor, means that most of the SMI handler virtualized, with only a small part called the STM serving as its hypervisor. I don't think is actually implemented in UEFI. PPAM - also called Nifty Rock or Devil's Gate Rock, tries to fill the gap between IRBR and STM by prepending an Intel entry-point to the SMI handler. Intel supplies a signed module called PPAM that can measure certain attributes of the SMI handler and report them to the OS. The OS can then make a policy decision on how to proceed. All SMI handler must also be registered in a table called the WSMT table. The firmware's WSMT tables declares to the OS that the firmware guarantees three things: FixedCommBuffers - a guarantee that the SMM will vaildate that the input/output buffers of the operation, CommBufferNestedPtrProtection that extends this guarantee to any pointers within input/output structures, and SystemResourceProtection that indicated that the SMI handler will not reconfigure the hardware. Memory Reset protections After a warm boot or even a fast cold boot some secrets (keys) might remain in memory. Intel provides security for these secrets using special TXT Secrets registers.</summary></entry><entry><title type="html">Starting a blog at this time</title><link href="https://igor-blue.github.io/2021/02/01/intro.html" rel="alternate" type="text/html" title="Starting a blog at this time" /><published>2021-02-01T00:00:00-05:00</published><updated>2021-02-01T00:00:00-05:00</updated><id>https://igor-blue.github.io/2021/02/01/intro</id><content type="html" xml:base="https://igor-blue.github.io/2021/02/01/intro.html">&lt;h1 id=&quot;is-this-a-good-time-to-start-a-new-cyber-security-blog&quot;&gt;Is this a good time to start a new cyber security blog?&lt;/h1&gt;

&lt;p&gt;I have been working in cybersecurity for quite some time, but have always been afraid of writing publicly about my work: afraid of being publicly rediculed for my work, afraid of my english proficiency and afraid in general.&lt;/p&gt;

&lt;p&gt;When I finally got the curage to start the North Korean blog thing happened, litterally a half hour before starting, throwing me way down.&lt;/p&gt;

&lt;p&gt;But finally I decided to go anyway.&lt;/p&gt;

&lt;p&gt;I’ll be posting a lot of my backlog in the next few weeks, including: the Intel PC boot process, intel uCode stuff, expereicnes as a blue teamer.&lt;/p&gt;

&lt;p&gt;Lets hope someone somewhere ever reads these words.&lt;/p&gt;

&lt;p&gt;Too-doo-loo,
Igor&lt;/p&gt;</content><author><name></name></author><summary type="html">Is this a good time to start a new cyber security blog?</summary></entry></feed>